<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[iptables]]></title>
    <url>%2F2018%2F12%2F20%2Fiptables%2F</url>
    <content type="text"><![CDATA[iptables的处理逻辑和命令 对于防火墙而言，原理和设置规则尤其重要，了解原理意味着清楚防火墙是如何在数据报文的必经之路上进行规则限制即iptables的处理逻辑； 规则意味着iptables是如何通过命令和各种选项进行规则编写的 对于防火墙而言，原理和设置规则尤其重要，原理意味着怎么对数据报文的走向进行规则限制，规则 Linux防火墙防火墙的概念iptables的基本认识iptables的组成:iptables的处理逻辑iptables的基本语法iptables之forward的概念iptables之地址转换法则SNAT源地址转换的具体实现DNAT目标地址转换的具体实现 不理解的地方：5个表的优先级和规则的优先级什么是显示扩展和隐式扩展 iptables的处理逻辑和命令开发环境测试环境 生产环境 安全技术：防火墙： 防范非授权网络访问；隔离功能，工作在网络或主机边缘，对进出网络或主机的数据包基 于一定的规则检查，并在匹配某规则时由规则定义的行为进行处理的一组功能的组件， 基本上的实现都是默认情况下关闭所有的通过型访问，只开放允许访问的策略 入侵检测机制和管理系统： 当病毒或者网络攻击绕过防火墙进入系统后的入侵检测机制，告知管理员采取手段提供有针对性的指导措施和安全决策依据。 杀毒软件：病毒入侵后的防御软件 入侵防御系统： 入侵检测系统检测到网络攻击或者病毒时，通过入侵防御系统进行准确的分析判断， 在判定为攻击行为后立即予以阻断，主动而有效的保护网络的安全，一般采用在线部署方式 防火墙作用：作用： 数据报文发到服务器的以太网卡的二种访问行为 1.linux中，数据报文从服务器的以太网卡经过内核空间走一遭，不与用户空间交互，再从网卡出去，穿墙而过 2.或者数据报文达到网卡，到达内核空间，进而与本地用户空间的进程进行通讯，大多数网络通讯发生的目标所在，如httpd的访问，vsftp通讯，ssh通讯 究竟数据报文会以哪种方式怎么选择，会根据主机内部的路由表来选择，内核从网卡拿到请求报文，对linux而言tcp/ip协议栈是在内核中 意味着报文的处理是在内核中处理的，也就是说防火墙必须工作在内核中，防火墙必须在内核中完成tcp/ip报文所流进的位置，用规则取检查，才能真正工作起来； 根据内核的网络协议栈处理逻辑，会先拆除IP层的封装，获取到目标IP，如果是则拆除ip地址的首部封装，获取传输层封装(tcp的源端口，目标端口，几个标记位等信息；) 这里所说端口是指进程地址，防火墙通常是在这个地方做限制的，只有通过防火墙设置的规则，才能进而与用户空间的应用程序进行数据报文的传输.]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>运维技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HTTP协议]]></title>
    <url>%2F2018%2F12%2F18%2FHTTP%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[HTTP协议和APACHE原理Apache官网：软件基金会组成：apache名下有很多项目，如HTTP，Kafka，tomcat等本文说的是HTTP SERVERapacheHTTP2.4的官方文档http2.4文档：安装，模块，指令等说明HTTP2.4官方指令参考文档指令参考文档 http协议:应用层协议,主流http/1.1版本http协议的版本区别http0.9、http1.0、http1.1和http2.0的版本区别 http/0.9: 只有一个GET命令，且只能回应.html文件格式，像.txt等都不支持 http/1.0:效率太低 1.支持cache, MIME(支持各种资源类型), method2.建立TCP三次握手之后,每个TCP连接只能发送一个请求，发送数据完毕，连接就关闭，如果还要请求其他资源，就必须再新建一个连接,效率太低3.引入了POST命令和HEAD命令，头部信息和4.http协议加了头信息,ip(tcp(http(data))),data可以是各种MIME的资源类型可以用 http/1.1:主流使用版本 1.http/1.1版本开始引入持久连接（persistent connection）,即TCP连接默认不关闭， 对于同一个域名，大多数浏览器允许同时建立6个持久连接，提高了请求的并发性 这里所说的是指一个域名最大6个连接，但是在一个网页上可以有很多域名，所以会有相当多的持久连接，实现高并发请求 2.引入了管道机制（pipelining），即在同一个TCP连接里，客户端可以同时发送多个请求，进一步改进了HTTP协议的效率3.新增方法：PUT、PATCH、OPTIONS、DELETE(删除功能通常不用)4.缺点： 同一个TCP连接三次握手之后，所有的数据通信是按顺序进行传输的。服务器只能顺序处理回应，当前面的数据比较大，传输比较慢时，后面的许多请求则会一直处于排队状态，就造成”队头堵塞”现象5.解决队头堵塞的办法： 为避免上述问题，两种方法：一是减少请求数，二是同时多开持久连接。网页优化技巧，如合并脚本和样式表、将图片嵌入CSS代码、域名分片（domain sharding）等6.HTTP 协议不带有状态，每次请求都必须附上所有信息。请求的很多字段都是重复的，浪费带宽，影响速度: 不带状态怎么理解？ http不像tcp拥有限状态机；http协议是没有状态的，刷新页面信息丢失，所以会通过cookie和session来弥补这个缺点 http/2.0：解决 HTTP/1.1效率不高的问题 1.头信息和数据体都是二进制，称为头信息帧和数据帧2.和http1.1区别：请求不需要排队，提高效率 复用TCP连接，在一个连接里，客户端和浏览器都可以同时发送多个请求或回应，且不用按顺序一一对应，避免了“队头堵塞“,此双向的实时通信称为多工（Multiplexing）3.引入头信息压缩机制（header compression）,头信息使用gzip或compress压缩后再发送；客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，不发送同样字段，只发送索引号，提高速度4.HTTP/2 允许服务器未经请求，主动向客户端发送资源，即服务器推送（server push）：http2.0的推送既有好处又有缺点(产生很多垃圾信息) HTTP工作机制 工作机制主要分为两步：请求和响应 http请求：http requesthttp响应：http response一次http事务：请求响应 Web资源：web resource一个网页由多个资源构成，打开一个页面，会有多个资源展示出来，但是每个资源都要单独请求。因此，一个“Web 页面”通常并不是单个资源，而是一组资源的集合 静态页面/文件：无需服务端做出额外处理; 服务器端是什么样传到客户端就是什么样，比如下面这些 比如文件后缀：.html, .txt, .jpg, .js, .css, .mp3, .avi 只不过浏览器有时会解析出来以好看的页面展示给用户 动态页面/文件：服务端执行程序，返回执行的结果 服务器放的资源和客户端看到的不一样，看到的不是代码本身，而是代码的执行结果 文件后缀：.php, .jsp ,.asp,.sh等 将上面的文件程序代码执行结果配合数据库，再通过http协议封装报文传给用户 提高HTTP连接性能 并行连接：通过开多个TCP连接发起并发的HTTP请求持久连接：keep-alive,长连接，通过一次TCP三次握手连接后，传输多个请求再关闭tcp连接，关闭连接可以有两种方式：1.是设置每次的tcp连接时间；2.是规定传输的请求个数，HTTP的配置文件中可以进行设置，如http1.1和http2.0都支持持久连接，管道化连接：通过共享TCP连接发起并发的HTTP请求复用的连接：交替传送请求和响应报文（实验阶段） HTTP协议的其他相关技术 1.URI：一般把URI认为是URLURI: Uniform Resource Identifier 统一资源标识，分为URL和URN 1.URN: Uniform Resource Naming，统一资源命名 如P2P、迅雷等下载使用的磁力链接是URN的一种实现，不是从一个服务器上下载，从互联网任何一个有资源的地方下载，没有体现出这个资源在互联网上的路径，URN只代表这个资源叫什么名 2.URL: Uniform Resorce Locator，统一资源定位符，用于描述某服务器某特定资源位置 如输入一个网址，能具体体现出这个资源在互联网上的位置 两者区别：URN如同一个人的名称，而URL代表一个人的住址。换言之，URN定义某事物的身份，而URL提供查找该事物的方法。URN仅用于命名，而不指定地址 URL的组成scheme://user:password@host:port/path;params?query#frag scheme:表示访问服务器获取资源时，使用的哪种协议,如http，https,ftp等user:用户，某些方案访问资源时需要的用户名password:密码，用户对应的密码，中间用：分隔Host:主机，资源宿主服务器的主机名或IP地址，一般写FQDN通过DNS解析port:端口,资源宿主服务器正在监听的端口号，很多方案有默认端口号，如80,8080/path:路径,服务器资源的本地名，由一个/将其与前面的URL组件分隔params:参数，指定输入的参数，参数为名/值对，多个参数，用;分隔，一般是键值对query:查询，传递参数给程序，如数据库，用？分隔,多个查询用&amp;分隔，类似sql语句的select查询功能frag:片段,一小片或一部分资源的名字，此组件在客户端使用，用#分隔 网站访问量 1.IP(独立IP)：即Internet Protocol,指独立IP数。 如果一个局域网内通过一个公网IP出去，则局域网内IP数只算一个 2.PV(访问量)： 即Page View, 页面浏览量或点击量，用户每次刷新即被计算一次，PV反映的是浏览某网站的页面数， PV与来访者的数量成正比，PV并不是面的来访者数量，而是网站被访问的页面数量 一般为了博客为了好看的访问量，都是统计PV量 3.UV(独立访客)：即Unique Visitor,访问网站的一台电脑为一个访客。一天内相同的客户端只被计算一次。可以理解成访问某网站的电脑的数量。 网站判断来访电脑的身份是通过来访电脑的cookies或者session来实现的。 如果更换了IP后但不清除cookies，再访问相同网站，该网站的统计中UV数是不变的 如：局域网内三个人用同一台电脑访问一个网站，则这个网站的 IP:1;PV:6;UV:1 Web服务请求处理步骤很切合实际生活：比如当我们打开一个浏览器，输入一个www.taobao.com，在互联网后台发生了什么？这就涉及到web服务请求处理步骤和一次完整的http请求处理过程了，如下图 当然在建立连接前还有DNS解析–&gt;TCP建立连接的三次握手—&gt;再到完整的http的处理过程每个过程都是一段需要原理详细描述的. 一次完整的http请求处理过程1.建立连接：接收或拒绝连接请求 2.接收请求：接收客户端请求报文中对某资源的一次请求的过程 Web访问响应模型（Web I/O） 单进程I/O模型：访问量不大 启动只有一个进程处理用户请求，而且一次只处理一个，多个请求被串行响应 会造成请求排队现象，只适用于访问并发不大的情况 多进程I/O模型： 系统开启多个进程处理用户请求,而且每个进程只响应一个连接请求 如果并发请求过高时，也会造成请求排队现象，只是比单进程稍微高效一点 而且服务器上开多个进程是要占用消耗资源的，CPU、内存的容量也是不支持的 复用I/O结构： 开启多个进程进程，而一个进程又同时监控N个连接请求 只有当N个连接请求真正进行数据传输时，这个进程才会响应，就不需要像多进程一样开启多个进程，极大的减小服务器的资源消耗 实现方法：多线程模型和事件驱动 多线程模型：一个进程生成N个线程，每线程响应一个连接请求 事件驱动：一个进程处理N个请求 复用的多进程I/O模型： 启动M个进程，每个进程响应N个连接请求，同时接收M*N个请求 充分使用CPU个数，每个核开启一个进程，一个进程响应多个请求，这就是典型的 nginx使用的复用多进程I/O模型 1.apache使用的就是多进程I/O模型，缺点：就是开启很多进程，用户量太大时，当达到并发连接数10k时，进程数太多，CPU、内存资源消耗严重，服务器性能下降，apache的响应速度就会变慢，不能支持高并发的连接请求，通过 ps auxf可以看出是一个父进程多个子进程 2.nginx使用的是复用的多进程I/O模型，CPU的每个核开启一个进程，一个进程负责N个连接请求，当这些连接请求真正进行数据传输时，这个进程才会响应 避免了多进程I/O模型来一个请求就开启一个进程的缺点，即使不进行数据传输也会占用一个进程的问题，会造成很大的资源占用浪费 同时也避免了复用I/O模型的缺点：N个进程同时进行数据传输时，一个进程无法同时相应的问题 参考下面的HTTP的MPM三种工作模式 3、处理请求：服务器对请求报文进行解析，并获取请求的资源及请求方法等相关信息，根据方法，资源，首部和可选的主体部分对请求进行处理 分析元数据：请求报文首部信息获得method &lt;method&gt; &lt;URL&gt; &lt;VERSION&gt; HEADERS 格式 name:value &lt;request body&gt; 通过method来响应用户请求，比如下载(get)，上传等信息 HTTP常用请求方式，Method GET、POST、HEAD、PUT、DELETE、TRACE、OPTIONS 4.访问资源： 服务器获取请求报文中请求的资源web服务器，即存放了web资源的服务器，负责向请求者提供对方请求的静态资源，或动态运行后生成的资源 在apache中默认的根/是---&gt; /var/www/html/index.html 可以修改配置文件 web服务器资源路径映射方式： (a) docroot (b) alias (c) 虚拟主机docroot (d) 用户家目录docroot 5.构建响应报文： 一旦Web服务器识别除了资源，就执行请求方法中描述的动作，并返回响应报文。响应报文中 包含有响应状态码、响应首部，如果生成了响应主体的话，还包括响应主体 1）响应实体 描述了响应主体MIME类型的Content-Type首部 描述了响应主体长度的Content-Length 2）URL重定向 web服务构建的响应并非客户端请求的资源，而是资源另外一个访问路径 3）MIME类型 当get到/var/www/html/index.html文件后，http会添加响应实体或者URL重定向，MIME类型信息 6.发送响应报文 将构建完的响应报文发送给用户 将index.html(构建完的报文)进行http、tcp、IP头等封装后的响应报文发给用户 用户再层层解封装获得index.html的内容 7.记录日志 最后，当事务结束时，Web服务器会在日志文件中添加一个条目，来描述已执行的事务 通过在/var/log/httpd/acess_log日志中记录http的响应记录数 方便分析日志统计该网站的IP，PV量等信息 HTTP介绍特性：高度模块化：core + modulesDSO: Dynamic Shared Object 动态加/卸载MPM：multi-processing module多路处理模块 HTTP的三种工作模型：MPM工作模式多用途的处理模块，三种模型分别是，默认使用prefork模型因为http很多软件是只支持prefork模型而不支持woker或者event模型的，所以一般都是使用prefork模型 可以参考Web访问响应模型（Web I/O）HTTP原理 1.prefork：多进程I/O模型，每个进程响应一个请求，默认模型 一个主进程：生成和回收n个子进程，创建套接字，不响应请求 多个子进程：工作work进程，每个子进程处理一个请求；系统初始时，预先生成多个空闲进程，等待请求，最大不超过1024个，消耗比较大内存资源 2.worker：复用的多进程I/O模型,多进程多线程，IIS使用此模型 一个主进程生成m个子进程，每个子进程又负责生个n个线程，每个线程响应一个请求，并发响应请求：m*n，支持高并发性 缺点：线程消耗资源比进程要小很多，但是如果子进程中的线程出现问题，则会影响整个子进程，从这个方面考虑，没有prefork稳定！ 3.event：事件驱动模型（worker模型的优化,worker模型的变种） event模型在httpd-2.4：event 稳定版，centos7默认版本开始使用 一个主进程：生成m个子进程，每个进程直接响应n个请求，并发响应请求：m*n 相比较worker的有点： 有专门的线程来监控管理这些keep-alive类型的线程，当有真实请求时，将请求传递给服务线程，执行完毕后，又允许释放。这样增强了高并发场景下的请求处理能力 进程工作的角色切换 Httpd的功能特性httpd的常见特性： 虚拟主机：在一个物理服务器上搭建多个网站 IP、Port、FQDN CGI：Common Gateway Interface，通用网关接口 通过CGI接口处理动态的程序处理 反向代理 当有用户访问量大时，在前端有一个服务器充当调度器的角色 通过调度算法把请求发送到后端真正提供服务的服务器上去，用户只看到调度器的信息 看不到后端真正提供服务的服务器群 负载均衡 路径别名 丰富的用户认证机制 basic digest 支持第三方模块 Httpd2.4新特性和安装以及配置新特性 MPM支持运行为DSO机制；以模块形式按需加载 在centos7上的httpd2.4上只有一个二进制程序 /usr/sbin/httpd，更改MPM模式只需要加载event或者worker模块即可 但是在centos6上的httpd2.2版本： 每个MPM模式都有各自对应的二进制程序 /usr/sbin/httpd /usr/sbin/httpd.event /usr/sbin/httpd.worker 如果更改MPM的模式，是需要改对应的二进制程序的 event MPM生产环境可用 异步读写机制 支持每模块及每目录的单独日志级别定义 每请求相关的专用配置 增强版的表达式分析式 毫秒级持久连接时长定义：httpd2.2只能精确到秒级别 上文提到http1.1和http2.0协议版本都支持持久连接，但是持久连接也是有时长的 所以在http中是可以定义连接时长(时长和传输请求两种方式)的 基于FQDN的虚拟主机不需要NameVirutalHost指令 httpd2.2创建虚拟主机时还需要NameVirutalHost指令 httpd2.4就不需要了 新指令，AllowOverrideList 支持用户自定义变量 更低的内存消耗 Httpd2.4的具体安装和配置Httpd的软件比较复杂，所以很多功能都模块化了，需要什么功能，加载模块就行了还支持第三方的模块，按需加载模块存放模块的路径：/etc/httpd/modules CentOS7程序环境安装：httpd-2.4 yum install httpd yum install httpd-manual 帮助文档，无网络环境下可以查看帮助文档 主要配置文件： /usr/sbin/httpd httpd的主程序文件 /usr/lib/systemd/system/httpd.service httpd的启动单元文件 /etc/httpd/conf/httpd.conf 主要配置文件 配置文件里的路径都是以/etc/httpd/为参考点的 /etc/httpd/conf.d/*.conf /etc/httpd/conf.modules.d/00-mpm.conf mpm相关 /etc/httpd/conf.modules.d/00-proxy.conf 配置代理相关 /etc/httpd/conf.modules.d/00-systemd.conf /etc/httpd/conf.modules.d/01-cgi.conf CGI相关 /var/cache/httpd httpd的缓存目录 /var/log/httpd httpd的日志文件目录 access_log: 访问日志 error_log：错误日志 /etc/httpd/modules httpd的模块存放路径，软件比较复杂 /usr/lib64/httpd/modules 也是httpd的模块存放路径 两个模块的路径实际上是软链接关系 /var/www/html httpd存放网页的目录：默认index.html 帮助文档包：在没有网络时查看帮助文档，建议安装 httpd-manual 检查配置文件语法： httpd –t 类似DNS的rndc语法检查功能 sudo：visudo功能 ansible:ansible -C|--check 执行前检查语法功能 cobbler 也有 httpd的控制和启动命令 systemctl enable|disable httpd.service systemctl {start|stop|restart|status|reload} httpd.service 备注：很多服务都有reload功能，但是需要注意的是修改或加载模块等操作 有时reload是无效的，只能restart服务 Httpd2.4的常见配置 1.显示服务器版本信息HTTP2.4官方指令参考文档指令参考文档 主要组成: Global Environment 全局配置 Main server configuration 一个服务器上搭建一个网站叫主服务器 virtual host 虚拟主机，一台主机上搭建多个网站 练习一下grep命令：只显示配置文件中的非注释的行(出去注释行和空行)egrep -v ‘^ #.|^$’ /etc/httpd/conf/httpd.conf 常见配置一般都在/etc/httpd/conf/httpd.conf的文件中没有的配置选项可以加在httpd.conf文件最后，也可以放在/etc/httpd/conf.d/*.conf 创建一个 .conf文件，比如test.conf下面的一些配置都是在/etc/httpd/conf.d/test.conf文件中添加 1.显示服务器版本信息 访问http://192.168.34.103 打开f12调试模式，可以看到回应头的信息中带有apache的版本信息 也可以通过curl -I http://192.168.34.103来显示头信息 可以看到头信息里有：Server: Apache/2.4.6 (CentOS)，显示出具体版本会是网站不安全 查看指令参考文档：修改ServerTokens选项建议使用：ServerTokens Prod，重启httpd服务，apache版本信息关闭了，加固安全 2.修改监听的IP和Port Listen [IP:]PORT (1) 省略IP表示为本机所有IP (2) Listen监听端口至少一个，可以有多个端口 (3) 监听端口可以绑定到服务器的特定IP上，只能通过这个IP才能使用此端口 test.conf添加listen端口： listen 172.18.132.151:6666 6666端口只能通过此IP才能访问 3.持久连接：默认KeepAlive是开启的 Persistent Connection：连接建立，每个资源获取完成后不会断开连接，而是继续等待其它的请求完成，默认关闭持久连接 断开条件：时间限制：以秒为单位， 默认5s，httpd-2.4的新特性支持毫秒级 副作用：对并发访问量大的服务器，持久连接要设置短一点，要不然会造成请求排队堵塞 折衷：使用较短的持久连接时间 设置： KeepAlive On|Off 设置持久连接开启或者关闭 KeepAliveTimeout 15 设置持久连接为15s 测试：telnet WEB_SERVER_IP PORT GET /index.html HTTP/1.1 Host: WEB_SERVER_IP host可以随便填写，但是在虚拟主机中是有区别的 4.MPM（ Multi-Processing Module）多路处理模块：prefork,worker,event 默认是prefork：/etc/httpd/conf.modules.d/00-mpm.conf 修改mpm的文件 建议还是使用prefork模块，因为后续的很多模块都依赖于prefork模块的 查看静态编译的模块 httpd -l 查看当前系统中httpd的静态编译及动态装载的加载的模块 httpd –M 动态模块加载：不需重启即生效 动态模块路径 /usr/lib64/httpd/modules/ 切换使用的MPM模块 在/etc/httpd/conf.modules.d/00-mpm.conf中 选择要启用的MPM相关的LoadModule指令即可 prefork的配置： StartServers 8 MinSpareServers 5 保留5个空闲子进程处理新的请求 MaxSpareServers 20 允许的最大空闲进程数 ServerLimit 256 最多进程数,最大20000，并发量建议最多10000 MaxRequestsPerChild 4000 子进程最多能处理的请求数量。在处理 MaxRequestsPerChild个请求之后,子进程将会被父进程终止，这时候子进 程占用的内存就会释放(为0时永远不释放） worker和prefork配置类似，只不过会有线程数量的限制 从下面ps auxf的进程图可以看书httpd是使用的prefork模型，而且因为端口是80，开启需要特殊权限，所以父进程是root,子进程是apache 5.DSO： Dynamic Shared Object 加载动态模块配置 /etc/httpd/conf/httpd.conf Include conf.modules.d/*.conf 配置指定实现模块加载格式： LoadModule &lt;mod_name&gt; &lt;mod_path&gt; 模块文件路径可使用相对路径： 相对于ServerRoot（默认/etc/httpd） 6.定义’Main’ server的文档页面路径：存放页面的主目录 主目录是由DocumentRoot指令来设置的 网页文件默认是存在/var/www/html/下的，此处可以自定义 在/etc/httpd/conf/httpd.conf中默认DocumentRoot是”/var/www/html” 如果要自定义主目录，还需要设置该目录为允许访问 在httpd2.2上是不需要设置权限访问的 但是在httpd2.4上是需要设置该目录允许访问的 修改格式如下： #DocumentRoot &quot;/var/www/html&quot; DocumentRoot &quot;/data/www&quot; &lt;directory /data/www&gt; Require all granted &lt;/directory&gt; 7.定义站点默认主页面 访问192.168.34.103默认显示的是index.html的内容， 这一项是由DirectoryIndex指令设置的 在/etc/httpd/conf/httpd.conf中有定义默认页面，修改即可 添加：DirectoryIndex haha.html，当然前提是存放页面的主目录下有这个html文件 此处需要了解httpd的权限和访问报错的相关问题和默认主页面：1.访问192.168.34.103跳出的页面默认是index.html页面，如果不存在，则去找conf.d/welcome.conf文件，即报错提示，如果两个都不存在，显示的是/var/www/html目录下的文件列表，因为在httpd.conf配置文件中，定义options indexes选项2.默认页面和默认访问目录都是可以通过指令来指定的3.如果没有index.html文件，没有welcome.conf文件，没有options indexes配置，访问时会报错You don’t have permission to access / on this server.的提示.4.上面三条在下面第12条配置别名时，可以体现的很明显5.所以如果自定义目录和用别名，有时是需要设置目录访问权限、设置目录对应的index.html文件或者跳转的html文件(如果该文件存在就不需要设置了)，设置options indexes选项 8.站点访问控制常见机制 可基于两种机制指明对哪些资源进行何种访问控制 访问控制机制有两种：1.客户端来源地址，2.用户账号 而文件系统路径：可以是 1.具体的目录；2.可以是具体到一个文件；3.可以是扩展的正则表达式匹配 示例： ‘‘’ 用的是扩展的正则表达式 通配符 &lt;Location /status&gt; 9.中”基于源地址”实现访问控制:Options和Allowoverride和基于IP的控制 (1) Options：后跟1个或多个以空白字符分隔的选项列 在选项前的+，- 表示增加或删除指定选项 常见选项： Indexes：指明的URL路径下不存在与定义的主页面资源相符的资源文件时，返回索引列表给用户,即显示/var/www/html/下的所有文件给用户看，比如阿里和清华镜像源就是这个机制 适用于下载网站和镜像网站，不适合电商网站 FollowSymLinks：允许访问符号链接文件所指向的源文件, None：全部禁用 All： 全部允许 Indexes使用示例：前提是定义站点默认主页面的index.html不存在，存在的话显示的 就是index.html的内容，适用于下载网站和镜像网站，不适合电商网站 &lt;directory &quot;/data/www&quot;&gt; options Indexes Require all granted &lt;/directory&gt; FollowSymLinks使用示例：文件列表显示链接(创建的软硬链接)文件 &lt;directory &quot;/data/www&quot;&gt; options Indexes FollowSymLinks Require all granted &lt;/directory&gt; 实现options indexes的前提：存放页面的主目录下没有index.html文件，但是此时即使加上 options Indexes选项，页面也不会列出主目录下的所有文件，因为httpd找不到主目录下的index.html文件则会找/etc/httpd/conf.d/下的welcome.conf文件，welcome.conf文件就是页面上经常看到的测试123页面，把welcome.conf文件改名或删除，就会显示主目录下的所有列表文件了. (2) AllowOverride allowoverride意思是：把options选项放到.htaccess的文件里，而不是放在httpd.conf中,只需要把.htaccess文件放到你需要控制的目录下并设置allowoverride权限即可 只对语句有效; allowoverride权限时卸载httpd的*.conf配置文件中的 AllowOverride All: .htaccess中所有指令都有效 AllowOverride None： .htaccess 文件无效 AllowOverride AuthConfig Indexes 除了AuthConfig 和Indexes的其它 配置allowoverride示例： /etc/httpd/conf.d/test.conf中添加访问控制的目录 &lt;directory /data/www&gt; allowoverride all --&gt;这条必须写，代表.htaccess文件中的options生效 Require all granted &lt;/directory&gt; 在/data/www/目录下，创建.htaccess文件，将options添加到.htaccess中 options indexes FollowSymLinks (3) 基于IP的访问控制: 无明确授权的目录，默认拒绝 允许所有主机访问：Require all granted 拒绝所有主机访问：Require all denied 控制特定的IP访问： Require ip IPADDR：授权指定来源的IP访问 Require not ip IPADDR：拒绝特定的IP访问 控制特定的主机访问： Require host HOSTNAME：授权特定主机访问 Require not host HOSTNAME：拒绝 HOSTNAME：FQDN：特定主机和domin.tld：指定域名下的所有主机 基于IP的访问控制示例： 如：1.只拒绝特定IP地址访问/data/www目录，其他IP可以看到主目录下的文件列表 除了/data/www/ceshi文件夹 &lt;directory /data/www&gt; options indexes &lt;RequireAll&gt; Require all granted Require not ip 192.168.34.107 &lt;/RequireAll&gt; &lt;/directory&gt; 2.设置/data/www目录为所有IP可访问，并显示目录文件列表，但是在目录下的ceshi子目录只允许192.168.34.107访问 &lt;directory /data/www&gt; options indexes Require all granted &lt;/directory&gt; &lt;directory /data/www/ceshi&gt; &lt;RequireAny&gt; Require all denied Require ip 192.168.34.107 &lt;/RequireAny&gt; &lt;/directory&gt; 10.日志设定：日志默认/var/log/httpd/下format官方说明文档 日志类型：访问日志(access_log)和错误日志(error_log) 日志的格式，先通过Logformat指令指定然后起个名，然后生成的日志再调用这个日志格式 在/etc/httpd/conf/httpd.conf中定义了访问日志和错误日志的存放路径和日志格式定义 LogFormat &quot;%h %l %u %t \&quot;%r\&quot; %&gt;s %b \&quot;%{Referer}i\&quot; \&quot;%{User-Agent}i\&quot;&quot; combined --&gt;由Logformat指令定义完起一个combined名 CustomLog &quot;logs/access_log&quot; combined再调用这个日志格式 ErrorLog &quot;logs/error_log&quot; 可以看出日志文件都是以/etc/httpd/为根的，所以写的是相对路径 日志中的格式各个项说明 %h 客户端IP地址 %l 远程用户,启用mod_ident才有效，通常为减号“-” %u 验证（basic，digest）远程用户,非登录访问时，为一个减号“-” %t 服务器收到请求时的时间 %r First line of request，即表示请求报文的首行；记录了此次请求的“方法”， “URL”以及协议版本 %&gt;s 响应状态码 %b 响应报文的大小，单位是字节；不包括响应报文http首部 %{Referer}i 请求报文中首部“referer”的值；即从哪个页面中的超链接跳转至当前页面的，通过referer可以分析出网页 是否被调用了，比如说广告生效了，或者说搭建的个人网站被别人盗链了，占用了自己的服务器资源，所以网站一般要加防盗链，防止资源被盗和占用服务器资源 %{User-Agent}i 指客户端的浏览器版本 11.设定默认字符集 一般不需要设置：基本上使用的是utf-8; AddDefaultCharset UTF-8 此为默认值 如果需要修改字符集，在test.conf或者httpd.conf下添加 AddDefaultCharset gb2312 即可 12.定义路径别名作用：默认访问的文件夹是设置的DocumentRoot文件夹，如果不是在默认文件夹下，而需求要实现把其他路径下的目录共享出来，这就用到了定义路径别名的功能 把一个URL起一个别名不指向真正的目录 在httpd2.4里目录如果没有开启允许时，默认是不允许访问的 例如：访问 /var/www/html/111,111这个目录可有可无，只是充当别名 实际上访问的是/data/www/ceshi目录 directoryindex ceshi.html alias /111 /data/www/ceshi &lt;directory /data/www/ceshi&gt; options indexes Require all granted &lt;/directory&gt; 注意：1.111只是一个别名，在/var/www/html下不需要创建；但是如果/var/www/html下有111的文件夹，如果设置别名后，则访问的是/data/www/ceshi目录，原目录下的111文件夹就失效了。2.在这里和上文第7条设置权限时有联系的，httpd2.4设置路径别名的时候，真正的路径也是需要在test.conf下设置权限的(/data/www/ceshi目录)3.如果/data/www/ceshi/下没有index.html会报权限不允许的错误，所以要设置默认主页面directoryindex，当然可以设置options indexes选项 13.基于虚拟账户的登录访问控制：提示401状态码 认证质询：WWW-Authenticate：响应码为401，拒绝客户端请求，并说明要求客户端提供账号和密码 认证：Authorization：客户端用户填入账号和密码后再次发送请求报文；认证通过时，则服务器发送响应的资源 两种认证方式： basic：用的多，缺点是明文，后面可以用https进行加密 digest：兼容性差，用的少 用户的账号和密码:非linux用户密码 虚拟账号：多种存储方式，只用于httpd服务的，类似vsftp，samba服务的虚拟用户 存储：账号可放在文本文件，SQL数据库，ldap目录存储，nis等 因为basic使用的多，下文以basic认证配置示例 1.提供账号和密码存储（文本文件），如创建一个tom虚拟账户管理httpd htpasswd --&gt;可以指定加密算法 -c 自动创建文件，仅应该在文件不存在时使用 -p 明文密码 -d CRYPT格式加密，默认 -m md5格式加密 -s sha格式加密 -D 删除指定用户 htpasswd -c httpdpass tom :设定密码即可;目录可指定，后面改配置文件即可 htpasswd -s httpdpass jerry 再添加jerry用户，就不需要-c选项了 htpasswd -D httpdpass jerry 从文件中删除jerry用户 修改httpdpass权限，加固安全 chmod 600 httpdpass 或者也可以创建组文件httpdgroup，通过组来管理,组文件组和成员列表 testgroup: tom jerry 2. 在test.conf或者httpd.conf下定义安全域 &lt;directory /var/www/html/ksdir&gt; AuthType Basic ---&gt;使用的认证方式 AuthName &quot;Login&quot; ----&gt;登录提示信息 AuthUserFile &quot;/etc/httpd/conf/httpdpass&quot; --&gt;加密账户文件 AuthGroupFile &quot;/etc/httpd/conf/httpdgroup&quot; --&gt;组账户 Require user tom ---&gt;允许用户访问的列表 Require group testgroup ---&gt;允许访问的组 &lt;/directory&gt; 但是：因为httpd线程是以apache运行的，而httpdpass是600权限，所以默认tom登录是没有权限的，让apache对httpdpass有读权限： setfacl -m u:apache:r httpdpass 即可 14.实现用户家目录的http共享;并实现账户机密访问 实现基础：基于模块mod_userdir.so实现 httpd -M |grep user 可以看出user加密的模块是已经加载的，直接改配置文件即可 在/etc/httpd/conf.d/userdir.conf文件中定义了家目录共享的格式，只需要启用就行了 实现步骤： 1. vim /etc/httpd/conf.d/userdir.conf &lt;IfModule mod_userdir.c&gt; UserDir enabled ---&gt;启用即可；默认不启用 UserDir public_html ---&gt;创建一个public_html文件 &lt;/IfModule&gt; 2.在test家目录下，创建public_html文件夹和public_html文件 su test mkdir public_html/ echo 23333 &gt; /public_html/public_html 3.因为httpd线程是apache身份登录的，所以要允许apache访问test用户的家目录 setfacl -m u:apache:x /home/test 4.设置test家目录访问权限及用户加密 &lt;directory /home/test/public_html&gt; authtype basic authname &quot;test home&quot; authuserfile &quot;/etc/httpd/conf/httpdpass&quot; --&gt;调用之前创建的虚拟账户文件即可 Require user haha &lt;/directory&gt; 5.http访问test家目录方式,即可用加密账户登录 192.168.34.103/~test 15.ServerSignature On|Off|EMail 作用：当用户访问的页面不存在的情况下服务器会产生错误文档，而当ServerSignature选项是打开的情况下，错误文档会将服务器的名字，apahce的版本信息，显示在页面上，某种程度上会对web服务器产生不安全因素，所以一般建议关闭 在配置文件添加一行： ServerSignature off 即可 16.status页面 作用：显示apache的工作状态，有助于判断apache是否正常工作 status页面功能是由下面这个模块实现的：httpd LoadModule status_module modules/mod_status.so 实现步骤：在配置文件添加 &lt;Location &quot;/status&quot;&gt; SetHandler server-status &lt;/Location&gt; ExtendedStatus ON #显示扩展信息 记录的信息包括：启动时间，线程数，等待连接数，cpu，进程，MPM等信息 在实际生产中，由于apache服务器很多，就可以通过curl 192.168.34.103/status来抓取页面，分析apache某个指标是不是正常的； 比如编写简单一个脚本： curl 192.168.34.103/status |grep &quot;Apache Status&quot; || systemctl start httpd 17.实现http的虚拟主机 作用：一个物理服务器上实现多台虚拟主机，至于为什么会用到虚拟主机，有可能是因为要上线的同一网站的有多个域名，却只有一个公网IP地址，也有可能是主机不多啊..，所以虚拟主机的价值就体现出来了… 实现虚拟主机有三种方式 基于ip：为每个虚拟主机准备至少一个ip地址，一个主机可以配多个内网IP地址 基于port：为每个虚拟主机使用至少一个独立的port；80,8081,8082等等 基于FQDN：为每个虚拟主机使用至少一个FQDN 当然这三种方式，基于IP和port的对于用户来说不太现实，一般都是通过输入域名方式来实现虚拟主机搭建. 1.基于IP的虚拟主机搭建：但是这种方式用的比较少 先准备三个网站目录，和在本机上添加三个IP地址 &lt;virtualhost 192.168.34.103&gt; DocumentRoot &quot;/data/asite&quot; &lt;directory /data/asite&gt; Require all granted &lt;/directory&gt; customlog /var/log/httpd/access_a.log combined &lt;/virtualhost&gt; &lt;virtualhost 192.168.34.200&gt; DocumentRoot &quot;/data/bsite&quot; &lt;directory /data/bsite&gt; Require all granted &lt;/directory&gt; customlog /var/log/httpd/access_b.log combined &lt;/virtualhost&gt; &lt;virtualhost 192.168.34.210&gt; DocumentRoot &quot;/data/csite&quot; &lt;directory /data/csite&gt; Require all granted &lt;/directory&gt; 重启服务即可 通过curl 192.168.34.103 curl 192.168.34.200 curl 192.168.34.210 即可获取到各自的index.html文件内容，对应的日志也都生成了 2.基于port的虚拟主机搭建，监听三个port即可;使用的不多 listen 8081 listen 8082 listen 8083 &lt;virtualhost *:8081&gt; servername &quot;www.a.com&quot; DocumentRoot &quot;/data/asite&quot; &lt;directory /data/asite&gt; Require all granted &lt;/directory&gt; customlog /var/log/httpd/access_a.log combined &lt;/virtualhost&gt; &lt;virtualhost *:8082&gt; servername &quot;www.b.cn&quot; DocumentRoot &quot;/data/bsite&quot; &lt;directory /data/bsite&gt; Require all granted &lt;/directory&gt; customlog /var/log/httpd/access_b.log combined &lt;/virtualhost&gt; &lt;virtualhost *:8083&gt; servername &quot;www.c.net&quot; DocumentRoot &quot;/data/csite&quot; &lt;directory /data/csite&gt; Require all granted &lt;/directory&gt; 通过本机IP+port获得不同网站的信息 curl 192.168.34.103:8081 curl 192.168.34.103:8082 curl 192.168.34.103:8083 3.基于FQDN的虚拟主机搭建：用的最多 前提：这里就不搭建DNS服务器了，使用本机的hosts文件进行域名解析 &lt;virtualhost *:80&gt; servername &quot;www.a.com&quot; DocumentRoot &quot;/data/asite&quot; &lt;directory /data/asite&gt; Require all granted &lt;/directory&gt; customlog /var/log/httpd/access_a.log combined &lt;/virtualhost&gt; &lt;virtualhost *:80&gt; servername &quot;www.b.cn&quot; DocumentRoot &quot;/data/bsite&quot; &lt;directory /data/bsite&gt; Require all granted &lt;/directory&gt; customlog /var/log/httpd/access_b.log combined &lt;/virtualhost&gt; &lt;virtualhost *:80&gt; servername &quot;www.c.net&quot; DocumentRoot &quot;/data/csite&quot; &lt;directory /data/csite&gt; Require all granted &lt;/directory&gt; customlog /var/log/httpd/access_c.log combined &lt;/virtualhost&gt; 测试：curl www.a.com curl www.b.cn curl www.c.net 就可获得各自的主页面信息从这telnet就可以理解出，如果在互联网通过域名访问时(同一IP)，返回的结果是不同网站的信息 [root@node7-1 ~]#telnet 192.168.34.103 80 Trying 192.168.34.103... Connected to 192.168.34.103. Escape character is &apos;^]&apos;. GET /index.html HTTP/1.1 HOST: www.a.com 当访问192.168.34.103时 HOST：即网站的域名，返回的结果就是其对应的页面信息在互联网上，一般是先在本地服务器上通过第三种方式搭建多个网站，然后再通过服务商的智能DNS解析，可以通过访问不同的域名，解析到IP地址，服务器根据输入的HOST：域名判断访问的是哪个网站，进而返回对应网站站点的信息，对于主机较少或者IP有限的环境中，使用http虚拟主机还是很方便的。]]></content>
      <categories>
        <category>HTTP协议</category>
      </categories>
      <tags>
        <tag>apache,web服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ansible]]></title>
    <url>%2F2018%2F03%2F06%2Fansible%2F</url>
    <content type="text"><![CDATA[运维自动化管理工具之Ansible 内容：1.软件发布环境机制优势对比 和 2.ansible的应用ansible的相关的文档ansible的中文权威指南：ansible中文指南Github上的ansible-galaxy示例：ansible-galaxy 其他相关运维管理工具使用方法：pssh的使用方法参照链接文章：psshsaltstack介绍及使用参照链接文章：saltstackpuppet介绍及使用参照链接文章：puppet 当下有许多的运维自动化工具( 配置管理 )，例如：Ansible、SaltStack、Puppet、Fabric 常用自动化运维工具 PSSH：适用于主机数量很少的环境(基础ssh的key验证) Ansible:python,Agentless,中小型应用环境(自带代理功能) Saltstack:python，一般需部署agent，执行效率更高 Puppet:ruby, 功能强大,配置复杂，重型,适合大型环境 Fabric：python，agentless Chef: ruby,国内应用少 Cfengine func 发布更新环境灰度发布：又叫金丝雀发布(核心概念：一次只发布一部分主机)比如：共100台生产服务器，先发布其中的10台服务器，这10台服务器就是灰度服务器 而软件更新实际上是将旧版本或者新版本创建一个软链接指向软件的工具路径 如： 软件路径为:/data/app 正在用的软件版本V1.0：/data/app1.0 更新的软件版本V2.0：/data/app2.0 则需要把删除原来的软链接：/data/app1.0---&gt;/data/app 创建新的软链接：/data/app2.0---&gt;/data/app 10台机器升级软件版本后，先上线进行，然后再逐渐把剩余的90台主机升级上线 发布步骤： 1.准备好部署各个阶段的工件，包括：构建工件，测试脚本，配置文件和部署清单文件。 2.从负载均衡列表中移除掉「金丝雀」服务器。 3.升级「金丝雀」应用（排掉原有流量并进行部署）。 4.对应用进行自动化测试。 5.将「金丝雀」服务器重新添加到负载均衡列表中（连通性和健康检查）。 6.如果「金丝雀」在线使用测试成功，升级剩余的其他服务器（否则就回滚）。 A/B Testing A/B测试是用来测试应用功能表现的方法，例如可用性、受欢迎程度、可见性等等。 A/B 测试通常用在应用的前端上，不过当然需要后端来支持。 优势与不足： 优势：用户体验影响小，灰度发布过程出现问题只影响少量用户 不足：发布自动化程度不够，发布期间可引发服务中断 预发布验证： 新版本的代码先发布到服务器,和线上环境配置相同，只是未接入调度器 灰度发布： 可以基于主机，用户或者业务，又细分为地区，VIP和普通用户 蓝绿发布：核心：主备两套环境定义：不停老版本，升级备环境版本然后进行测试。确认OK后将流量切到新版本，然后老版本同 时也升级到新版本 主：绿色环境-活动环境：负责对外提供服务，版本：v1.0 备：绿色环境-非活动环境：版本：v2.0 工作机制： 先把备环境升级v1.0---&gt;v2.0版本，然后上线 把主环境的v1.0版本下线，已经升级的备环境进行替换 特点： 蓝绿部署无需停机，并且风险较小. 注意事项： 1.需要提前考虑数据库与应用部署同步迁移/回滚的问题 2.蓝绿部署需要有基础设施支持 3.在非隔离基础架构（ VM 、 Docker 等）上执行蓝绿部署，蓝色环境 和绿色环境有被摧毁的风险. 优势与不足： 优势：升级切换和回退速度非常快 不足：需要有2套基础设施,v2.0版本有问题，则对用户体验有直接影响 滚动发布：在灰度发布的基础上进行进一步优化定义： 一般是取出一个或者多个服务器停止服务，执行更新，并重新将其投入使用。周而复始，直到集群中所有的实例都更新成新版本，是自动化程度较高的发布方式. 特点： 1.这种部署方式相对于蓝绿部署，更加节约资源——它不需要运行两个集群、两倍的实例数. 可以部分部署，例如每次只取出集群的20%进行升级。 2.滚动式发布需要比较复杂的发布工具和智能LB,支持平滑的版本替换和流量拉入拉出 优势和不足: 优势：用户体验影响小，体验较平滑 不足：发布和回退时间比较缓慢。 发布工具比较复杂，LB需要平滑的流量摘除和拉入能力 滚动发布目前成熟型技术组织所采用的主流发布方式 Ansible ansible特性：-最多管理500台主机，更多效率会降低1.模块化：调用特定的模块，完成特定任务 -类似linux中的小命令 2.有Paramiko，PyYAML，Jinja2（模板语言）三个关键模块 3.支持自定义模块 4.基于Python语言实现 5.部署简单，基于python和SSH(默认已安装)，agentless(没有自己的代理服务) 6.安全，基于OpenSSH 7.支持playbook编排任务 -类似于脚本功能，多个脚本的集合成为Roles 8.幂等性： 一个任务执行1遍和执行n遍效果一样，不因重复执行带来意外情况 9.无需代理不依赖PKI（无需ssl） 10.可使用任何编程语言写模块 11.YAML格式，编排任务，支持丰富的数据结构 12.较强大的多层解决方案 Ansible的学习过程：1.ansible基本命令使用 2.ansible常用模块详解，介绍ansible单个命令的使用 3.YAML语法介绍 4.ansible playbook基础：剧本初体验，类似于写脚本 5.playbook中的变量：tags，handlers使用 6.plsybook模板：templates 7.playbook的条件判断：when 8.playbook的字典：with_items 9.Ansible Roles -多个playbook的组合，类似于多个脚本的集合 会在playbook的同目录下生成一个*.retry的文件记录会记录执行时的错误IP地址。 ansible命令执行过程ansible命令执行过程 1. 加载自己的配置文件 默认/etc/ansible/ansible.cfg 2. 加载自己对应的模块文件，如command 3. 通过ansible将模块或命令生成对应的临时py文件，并将该 文件传输至远程 服务器的对应执行用户$HOME/.ansible/tmp/ansible-tmp-数字/XXX.PY文件 4. 给文件+x执行 5. 执行并返回结果 6. 删除临时py文件，sleep 0退出 执行状态：(颜色定义在/etc/ansible/ansible.cfg中) 绿色：执行成功并且不需要做改变的操作 黄色：执行成功并且对目标主机做变更 红色：执行失败 CMDB作用介绍:CMDB:Configuration Management Database 配置管理数据库 将服务器的配置，网络配置写到数据库里 CMDB即配置管理数据库，通过识别、控制、维护，检查企业的IT资源，从而高效控制与管理不 断变化的IT基础架构与IT服务，并为其它流程，例如事故管理、问题管理、变更管理、发布管 理等流程提供准确的配置信息. 了解更多CMDB可参照文章：CMDB 1.ansible基本命令使用ansible软件安装：多种安装方法1.基于epel源安装： yum install ansible,非服务，只是一个管理工具 2.编译安装： 3.Github方式安装：可以同步安装 4.pip安装：pip是安装Python包的管理器，类似yum ansible的重要&amp;主要文件配置文件： /etc/ansible/ansible.cfg 配置ansible的工作特性 /etc/ansible/hosts 主机清单 /etc/ansible/roles 存放的角色目录 程序文件： /usr/bin/ansible ansible的可执行命令--&gt;2.7的软链接，都是软链接的思想 /usr/bin/ansible-doc 查看配置文档，模块功能查看工具，man帮助 /usr/bin/ansible-galaxy 下载/上传优秀代码或Roles模块的官网平台 /usr/bin/ansible-playbook 管理执行编排的playbook剧本任务 /usr/bin/ansible-vault 文件加密工具 /usr/bin/ansible-console 基于Console界面与用户交互的执行工具 常用命令： ansible all --list 查看ansible管理的主机群 ansible all -m &quot;模块名&quot; -a &apos;执行的命令&apos; 用什么模块执行什么命令 all也可以换成定义的--list中组的名字 ansible-playbook -C &apos;*.yml&apos; 对所有主机执行剧本 ansible-plsybook -C -t &apos;标签&apos; &apos;*.yml&apos; 对所有主机执行哪个标签的剧本 ansible主机清单配置：/etc/ansible/hosts,第一步(下文中介绍主机清单的多种表示方法)支持不分组，分组，等方式 如： 192.168.34.100 [webservers] 192.168.34.101 192.168.34.102 [dbservers] 192.168.34.[1:6]7 (17,27..67) db[01:100].cenntos.com ansible配置文件：/etc/ansible/ansible.cfg （一般保持默认）配置文件只提供默认值，但可以通过playbook的设置进行覆盖 配置文件可以放在/etc/ansible/ansible.cfg中 也可以放到一个工作目录下命名为.ansible.cfg [defaults] inventory = /etc/ansible/hosts - 主机列表配置文件 library = /usr/share/my_modules/ - 库文件存放目录 remote_tmp = $HOME/.ansible/tmp -临时py命令文件存放在远程主机目录 local_tmp = $HOME/.ansible/tmp - 本机的临时命令执行目录 forks = 5 - 默认并发数 sudo_user = root - 默认sudo 用户 ask_sudo_pass = True -每次执行ansible命令是否询问ssh密码 ask_pass = True remote_port = 22 host_key_checking = False -检查对应服务器的host_key，建议取消注释 log_path=/var/log/ansible.log -建议启用日志文件，利于排错 [color] 定义ansible命令的执行结果颜色的 配置文件说明和建议修改的项：local_tmp和remote_tmp： 本地临时文件和远程临时文件：把playbook转化成python程序，先放在本地 家目录的.ansible/tmp下，然后再通过ssh协议复制到远程主机的.ansible/tmp下，执行完毕后自动删除. host_key_checking = False -检查对应服务器的host_key，建议取消注释 log_path=/var/log/ansible.log -建议启用日志文件，利于排错 module_name = command -默认使用的命令模块，可以修改成shell module_name = shell 2.ansible常用模块详解，介绍ansible单个命令的使用ansible模块的使用查询方法ansible-doc: 显示模块帮助 ansible-doc [options] [module...] -a 显示所有模块的文档 -l, --list 列出可用模块 -s, --snippet显示指定模块的playbook片段 示例： ansible-doc –l 列出所有功能模块 ansible-doc ping 查看ansible中的ping用法 ansible-doc -s shell 查看shell模块的使用方法 ansible的常用基本选项ansible通过ssh实现配置管理、应用部署、任务执行等功能，建议配置ansible 端能基于密钥认证的方式联系各被管理节点 ansible语法： ansible &lt;host-pattern&gt; [-m module_name] [-a args] --version 显示版本 -m module 指定模块，默认为command，主要使用选项 -v 详细过程 –vv -vvv更详细 --list-hosts 显示主机列表，可简写 --list -k, --ask-pass 提示输入ssh连接密码，默认Key验证 -K, --ask-become-pass 提示输入sudo时的口令 -C, --check 检查，并不执行 -T, --timeout=TIMEOUT 执行命令的超时时间，默认10s -u, --user=REMOTE_USER 执行远程执行的用户 -b, --become 代替旧版的sudo 切换 ansible的主机清单表示方法:Host-pattern1.All ：表示所有Inventory中的所有主机 如：ansible all -m ping ansible all --list-hosts列出所有主机清单 ansible &quot;dbservers&quot; --list-hosts列出db组中的所有主机IP 2.* :通配符 如：ansible &quot;*&quot; = ansible all ansible 192.168.34.* 表示34网段的所有IP 3.或的关系 如：ansible &quot;websrvs:appsrvs&quot; 对两个组的主机执行操作 ansible &quot;192.168.1.10:192.168.1.20&quot;对两个主机执行操作 4.与的关系(且) 如：ansible &quot;websrvs:&amp;dbsrvs&quot;既在websrvs组并且在dbsrvs组中的主机 5.非，取反 如：ansible &apos;websrvs:!dbsrvs&apos; 在websrvs组，但不在dbsrvs组中的主机;注意：此处为单引号 6.正则表达式 如：ansible &quot;~(web|db).*\.centos\.com&quot; ansible的常见模块(第二步：具有很多模块，先列出比较常见且重要的，遇到再更新新的模块)ansible-doc +模块名 可以查看具体使用方法：显示都有哪些选项1.Command：在远程主机执行命令，默认模块，可忽略-m选项 可以在ansible.cfg中修改默认模块项 支持：chdir(切换目录) command模块不支持 $ &lt; &gt; | ; &amp; 等，可以用shell模块实现 使用示例： ansible all -a &apos;rm -f /data/f1&apos; 删除远程所有主机下f1文件，类似pssh ansible all -a &apos;useradd test&apos; 所有主机上创建test用户 2.Shell：和command相似，用shell执行命令，执行时要加-m shell选项 支持功能：支持$ &lt; &gt; | ; &amp; 等 chdir 执行前，先切换到该文件夹 示例:ansible appsrvs -m shell -a &apos;echo $HOSTNAME&apos; 显示appsrvs组的主机名 ansible all -m shell -a &apos;chdir=/data rm -rf *&apos; 先切换到/data目录下，再执行删除命令 3.Script: 批量运行脚本 可以现在管理机上先把复杂命令写成脚本，再通过script去批量管理 功能：creates:远程主机的文件存在，则不运行 removes:远程主机的文件不存在，则也不运行==即文件存在，则执行命令 示例：ansible all -m script -a &quot;/data/test.sh&quot; ansible all -a &quot;creats=/etc/fstab rm -rf /data/*命令&quot; 因为fstab文件存在，则不执行rm -rf /data/*命令 ansible all -a &quot;removes=/etc/fstab rm -rf /data/*&quot; 因为fstab文件存在，则执行rm -rf /data/*命令 4.Copy:从服务器复制文件到目标主机 src,dest,mode,owner,content,backup,mode 示例：1.nsible all -m copy -a &apos;src=/etc/fstab dest=/data/fstab2 backup=yes owner=test&apos; 将fstab拷贝到主机被改名为fstab2，如果存在先备份，所有者改为test用户 2.ansible all -m copy -a &apos;content=&quot;selinux1\nselinux2&quot; dest=/data/selinux1&apos; 根据自己写的字符串生成文件 5.Fetch:从目标主机抓取文件至服务器端，copy相反，目录可先tar再抓取 src，dest(抓取到本机目录) 示例：ansible all -m fetch -a &apos;src=/data/fstab2 dest=/data/&apos; 将远程主机fstab2文件抓取到本机/data下 如果抓取的是目录，先打包再抓取 打包：ansible all -a &apos;tar cf /root/data.tar /data&apos; 抓取：ansible all -m fetch -a &apos;src=/data/fstab2 dest=/data/&apos; 6.File：设置文件属性，创建/删除文件 src(创建链接文件),path=dest,state：(touch,absent,link,direcotory) 示例：创建文件：ansible all -m file -a &apos;dest=/data/f10 state=touch&apos; 创建文件夹：ansible all -m file -a &apos;dest=/data/haha state=directory&apos; 删除文件夹/目录：ansible all -m file -a &apos;dest=/data/haha state=absent&apos; 7.Hostname：管理主机名 可以通过后面的变量来实现 a.先在hosts后定义hostname变量名 [centos6] 192.168.34.106 hostname=mini6-2 192.168.34.101 hostname=node6-1 [centos7] 192.168.34.107 hostname=mini7-1 b.再通过hostname模块批量修改 ansible all -m hostname -a &apos;name={{hostname}}&apos; 8.Cron：计划任务 支持：minute，hour，day，month，weekday 示例：ansible all -m cron -a &apos;minute=*/5 job=&quot;/usr/sbin/ntpdate 172.18.0.1 &amp;&gt;/dev/null&quot; name=Synctime&apos; 创建计划任务 ansible srv -m cron -a &apos;name=Synctime state=absent&apos; 删除指定的计划任务名 ansible srv -m cron -a &apos;name=Synctime disable=true&apos; 禁用指定的计划任务名 9.Yum：管理包 支持：name,state=(started stopped reloaded restarted),absent 更新缓存：update_cache=yes， 示例：ansible all -m yum -a &apos;name=httpd,samba,vsftpd&apos;安装多个包 ansible all -m yum -a &apos;name=httpd,samba,vsftpd state=absent&apos;删除包 10.Service：管理服务(同一systemctl&amp;service) name，state(stopped,started,reloaded,restarted) enable(设置开启启动) 示例：ansible srv -m service -a &apos;name=httpd state=stopped&apos; 停止服务 ansible srv –m service –a &apos;name=httpd state=reloaded&apos; 重新加载服务 ansible srv -m service -a &apos;name=httpd state=restarted&apos; 重启服务 ansible srv -m service -a &apos;name=httpd enable=yes&apos; 设置开机启动 11.User：管理用户 name,comment(描述)，group(主组),groups(附加组)，uid，home，shell,system(系统用户) (absent,remove删除用户及家目录) 示例：ansible all -m user -a &apos;name=user1 uid=234 home=/data/user1 system=yes group=root groups=bin shell=/sbin/nologin comment&apos; 创建用户，指定uid，家目录，主组，附加组，shell类型，指定为系统用户 ansible all -m user -a &apos;name=user1 state=absent remove=yes&apos;删除用户及家目录 12.Group：管理组 支持：group,name,gid,system,state=(absent) 示例：ansible srv -m group -a &apos;name=testgroup system=yes&apos; 创建系统组 ansible srv -m group -a &apos;name=testgroup state=absent&apos; 删除组 ansible系列的一些模块(用的不多)简单介绍与了解： ansible-galaxy 互联网上的角色分享 ansible-pull 推送命令至远程，效率无限提升，对运维要求较高 Ansible-vault管理yaml文件 功能：管理加密解密yml文件 ansible-vault [create|decrypt|edit|encrypt|rekey|view] ansible-vault encrypt hello.yml 加密 ansible-vault decrypt hello.yml 解密 ansible-vault view hello.yml 查看 ansible-vault edit hello.yml 编辑加密文件 ansible-vault rekey hello.yml 修改口令 ansible-vault create new.yml 创建新文件 Ansible-console ansible重要知识之playbook(上面的各种模块的组合) YAML语言（编写playbook的专门语言）YAML语法： 在单一档案中，可用连续三个连字号(——)区分多个档案。另外，还有选择性的连续三 个点号( ... )用来表示档案结尾 次行开始正常写Playbook的内容，一般建议写明该Playbook的功能 使用#号注释代码 缩进必须是统一的，不能空格和tab混用 缩进的级别也必须是一致的，同样的缩进代表同样的级别，程序判别配置的级别是通过 缩进结合换行来实现的 YAML文件内容是区别大小写的，k/v的值均需大小写敏感 k/v的值可同行写也可换行写。同行使用:分隔 v可是个字符串，也可是另一个列表 一个完整的代码块功能需最少元素需包括 name: task 一个name只能包括一个task YAML文件扩展名通常为yml或yaml List：列表，其所有元素均使用“-”打头 Dictionary：字典，通常由多个key与value构成 Playbook中的核心元素:1.Hosts 执行的远程主机列表 2.remote_user: 可用于Host和task中。也可以通过指定其通过sudo的方式在远 程主机上执行任务，其可用于play全局或某任务；此外，甚至可以在sudo时使 用sudo_user指定sudo时切换的用户 3.Tasks 任务集 4.Varniables 内置变量或自定义变量在playbook中调用 5.Templates 模板，可替换模板文件中的变量并实现一些简单逻辑的文件 6.Handlers 和notity结合使用，由特定条件触发的操作，满足条件方才执行，否 则不执行 7.tags 标签 指定某条任务执行，用于选择运行playbook中的部分代码。ansible 具有幂等性，因此会自动跳过没有变化的部分，即便如此，有些代码为测试其 确实没有发生变化的时间依然会非常地长。此时，如果确信其没有变化，就可 以通过tags跳过此些代码片断 8.handlers和notify 运行playbook运行playbook的方式 ansible-playbook &lt;filename.yml&gt; ... [options] 常见选项 -C|--check 只检测可能会发生的改变，但不真正执行操作,建议执行前先检测 --list-hosts 列出运行任务的主机 --limit 主机列表 只针对主机列表中的主机执行 -v 显示过程 -vv -vvv 更详细 备注： 执行前建议 ansible-plsybook -C install_httpd.yml检查语法错误 ansible-playbook install_httpd.yml --list-hosts可以查看对哪些主机执行 执行playbook时，更改正确的是绿色的，更改的显示黄色的，错误信息显示红色，可以根据颜色变化判断是否更改，或者是否更改成功了。示例：以下实验均在centos7管理centos7集群，因为6&amp;7配置文件不同，均在7上实验将centos7的httpd.conf复制到centos7主机，6上的配置文件不同示例1：写一个安装启动httpd的playbook:install_httpd.yml 包括创建用户，安装httpd包，开启服务，并设置开机启动 - hosts: all remote_user: root tasks: - name: creat user user: name=httpd shell=/sbin/nologin uid=1234 home=/data/httpd - name: copy config copy: src=/data/httpd.conf dest=/etc/httpd/conf/httpd.conf - name: install package yum: name=httpd - name: service service: name=httpd state=started enabled=yes 备注： 执行完通过以下命令判断每个任务都否都执行成功了 1.ansible all -a &apos;getent passwd httpd&apos; 2.ansible all -a &apos;rpm -q httpd&apos; 3..ansible all -a &apos;ss -ntlp|grep 80&apos; 示例2：写一个删除上面的playbook:remove_httpd.yml 包括：删除用户，卸载httpd包 - hosts: all remote_user: root tasks: - name: del user user: name=httpd state=absent remove=yes - name: remove package yum: name=httpd state=absent 备注： 如果只删除特定主机的httpd，而不是全部，需要加--limit选项 ansible-playbook --limit 192.168.34.105 remove_httpd.yml 只限制在192.168.34.105的主机执行 上面的playbook只是实现了简单的安装配置功能，但是不能根据在更改配置文件后，再次执行，因为服务设置是start，不合理，所以要用到下面触发条件，来达到更改控制的目的。handlers和notify结合使用触发条件:当达到某个条件时，触发执行对应的task任务。Handlers: 是task列表，这些task与前述的task并没有本质上的不同,用于当关注的资源发生 变化时，才会采取一定的操作 Notify: 此action可用于在每个play的最后被触发，这样可避免多次有改变发生 时每次都执行指定的操作，仅在所有的变化发生完成后一次性地执行指定操作。 在notify中列出的操作称为handler，也即notify中调用handler中定义的操作 示例：示例3：将memcached的配置文件的端口更改后再复制到各主机上，服务需要重启，则用到了handlers和notify功能 （端口11211改成11200） - hosts: all remote_user: root tasks: - name: creat user user: name=memcached shell=/sbin/nologin uid=2345 - name: install package yum: name=memcached - name: copy config copy: src=/data/memcached dest=/etc/sysconfig/memcached notify: restart service 和handlers名称一致 - name: service service: name=memcached state=started enabled=yes handlers: - name: restart service 和notify名称一致 service: name=memcached state=restarted 备注：停止并删除用户和安装包 ansible all -a &apos;service memcached stop&apos; ansible all -a &apos;ss -ntl&apos; ansible all -a &apos;rpm -q memcached&apos; ansible all -a &apos;getent passwd memcached&apos; 可以多个notify对应一个handlers，也可以多个motify对应多个handlers示例4：多个notify对应一个handlers - hosts: websrvs remote_user: root tasks: - name: Install httpd yum: name=httpd state=present - name: Install configure file copy: src=files/httpd.conf dest=/etc/httpd/conf/ notify: restart httpd 第一个notify - name: ensure apache is running service: name=httpd state=started enabled=yes notify: restart httpd 第二个notify handlers: - name: restart httpd 对应一个handlers service: name=httpd status=restarted 示例5：多个notify对应多个handlers- hosts: websrvs remote_user: root tasks: - name: config copy: src=/root/config.txt dest=/etc/nginx/nginx.conf notify: - Restart Nginx - Check Nginx Process 多个notify的写法 handlers: - name: Restart Nginx 对应写多个handlers service: name=nginx state=restarted enabled=yes - name: Check Nginx process shell: killall -0 nginx &gt; /tmp/nginx.log tags的用法：作用：挑选某一段的task来执行将安装memcached的yml，在拷贝的动作后加一个标签,执行时指定标签运行 然后执行：ansible-plsybook -t ceshi install_memcached.yml 只会触发拷贝文件和handlers的动作 --- #test yaml file - hosts: all remote_user: root tasks: - name: creat user user: name=memcached shell=/sbin/nologin uid=2345 - name: install package yum: name=memcached - name: copy config copy: src=/data/memcached dest=/etc/sysconfig/memcached notify: restart service 和handlers名称一致 tags: ceshi 对拷贝动作加一个标签 - name: service service: name=memcached state=started enabled=yes handlers: - name: restart service 和notify名称一致 service: name=memcached state=restarted Playbook中变量使用:可以多出定义，但是存在优先级优先级的顺序为：-e var &gt; yaml中的var &gt; hosts中的普通变量 &gt; hosts公共变量变量名：仅能由字母、数字和下划线组成，且只能以字母开头 变量来源： 1 ansible setup facts 远程主机的所有变量都可直接调用 setup是一个模块，收集所有主机的各种信息，如果要用变量，需要先在里面找出对应的 代码块，然后用代码块当变量 比如：ansible all -m setup | grep &quot;version&quot; 过滤和版本有关的 ansible all -m setup | grep &quot;name&quot; 过滤和主机名有关的 2 在/etc/ansible/hosts中定义 普通变量：主机组中主机单独定义，优先级高于公共变量 公共（组）变量：针对主机组中所有主机定义统一变量 3 通过命令行指定变量，优先级最高 可以对单个变量赋值：ansible-playbook –e varname=value 也可以对多个变量赋值：ansible-playbook –e &quot;var1=1 var2=2&quot; 4 在playbook中定义 vars: - var1: value1 - var2: value2 5 在独立的变量YAML文件中定义，即roles下的var目录下的var.yml文件 很适合在roles中进行单独定义 6 在role中定义（下文中有介绍） 从setup模块中查找到的有用的变量；可以通过判断变量的值，然后执行不同的操作，类似于shell中，先判断version==7？然后再执行不同的命令#####先列出setup中几个有用的变量,然后在plsybook中执行不同操作 ansible_fqdn 主机名的变量 ansible_hostname 主机名 ansible_distribution_major_version: “6” 版本名变量 ansible_processor_vcpus 虚拟cpu个数变量 ansible_memtotal_mb 内存的变量 示例： ansible all -m setup -a “filter=ansible_memtotal_mb” 用此命令来查看系统内变量的值 调用不同变量来源的示例：得出变量的优先级顺序示例1：调用setup中的ansible_hostname主机名变量，来生成对应文件 var.yml - hosts: all remote_user: root tasks: - name: touch file file: name=/data/{{ ansible_hostname }}.log state=touch 示例2：将变量定义在/etc/ansible/hosts中(濮普通变量和公共变量)，然后调用变量 /etc/ansible/hosts：中定义的变量： [websrvs] 192.168.34.105 port1=80 192.168.34.106 port1=90 -普通变量 [websrvs:vars] -公共组变量 mark=&quot;-&quot; [appsrvs] 192.168.34.101 port1=100 [appsrvs:vars] mark=&quot;=&quot; vars.yml中书写格式： - hosts: all remote_user: root tasks: - name: touch file file: name=/data/app{{mark}}{{ port1 }}.log state=touch 最后生成的文件为： app=100.log，app-80.logapp-90.log 示例3：在示例1的基础上，再通过命令行中定义变量: 在外部定义ansible_hostname=&quot;hahaha&quot;，对比示例1的执行结果： ansible-playbook -e ansible_hostname=&quot;hahaha&quot; vars.yml 可以看出，最后新建的文件名为hahaha.log 示例4：在playbook中定义变量 - hosts: all remote_user: root vars: - port1: 200 - mark: +++ tasks: - name: touch file file: name=/data/app{{mark}}{{ port1 }}.log state=touch 生成的文件： app+++200.log 示例5：先写在var.yml中定义变量， 1.先准备cat vars.yml:文件内容格式 var1: httpd var2: nginx 2.在cat var.yml，中调用准备好的vars.yml文件这种方式适用于在roles中单独定义 - hosts: web remote_user: root vars_files: - vars.yml tasks: - name: create httpd log file: name=/app/{{ var1 }}.log state=touch - name: create nginx log file: name=/app/{{ var2 }}.log state=touch 模板templates，作用：文本文件，嵌套有脚本（使用模板编程语言编写） Jinja2语言，使用字面量，有下面形式 字符串：使用单引号或双引号 数字：整数，浮点数 列表：[item1, item2, ...] 元组：(item1, item2, ...) 字典：{key1:value1, key2:value2, ...} 布尔型：true/false 算术运算：+, -, *, /, //, %, ** 比较操作：==, !=, &gt;, &gt;=, &lt;, &lt;= 逻辑运算：and, or, not 流表达式：For If When templates功能：根据模块文件动态生成对应的配置文件 templates文件必须存放于templates目录下，且命名为 .j2 结尾 yaml/yml 文件需和templates目录平级，目录结构如下： ./ ├── temnginx.yml └── templates └── nginx.conf.j2 示例1：通过templates模板nginx1.先生成nginx.conf.j2模板 cp /etc/nginx/nginx.conf templates/nginx.conf.j2 2.创建playbook - hosts: all remote_user: root tasks: - name: inastll nginx yum: name=nginx - name: template template: src=nginx.conf.j2 dest=/etc/nginx/nginx.conf notify: service - name: start service service: name=nginx state=started handlers: - name: service service: name=nginx state=restarted when配合templates实现根据不同版本执行不同的功能条件测试: 如果需要根据变量、facts或此前任务的执行结果来做为某task执行与 否的前提时要用到条件测试,通过when语句实现，在task中使用，jinja2的语法 格式 when语句 在task后添加when子句即可使用条件测试；when语句支持Jinja2表达式语法 示例： tasks: - name: &quot;shutdown RedHat flavored systems&quot; command: /sbin/shutdown -h now when: ansible_os_family == &quot;RedHat&quot; 非赋值，而是比较是否为某个值 示例2：通过templates模板根据不同的centos版本，安装不同的httpd,就用到了when步骤：涉及到多个notify对应一个handlers,定义端口变量 1.hosts文件配置：修改了4台主机httpd的端口 [centos6] 192.168.34.105 http_port=86 192.168.34.106 http_port=87 192.168.34.101 http_port=88 [centos7] 192.168.34.107 http_port=89 2.将centos6&amp;centos7的httpd配置文件复制到templates/并改名为*.j2文件 httpd_6.conf.j2 httpd_7.conf.j2 3.将端口都自定义：修改httpd_6.conf.j2和httpd_7.conf.j2的 Listen {{http_port}} 调用hosts列表中的端口变量 4.plsybook如下： --- - hosts: all remote_user: root tasks: - name: install httpd yum: name=httpd - name: templates 6 template: src=httpd_6.conf.j2 dest=/etc/httpd/conf/httpd.conf notify: restart service when: ansible_distribution_major_version == &quot;6&quot; - name: templates 7 template: src=httpd_7.conf.j2 dest=/etc/httpd/conf/httpd.conf when: ansible_distribution_major_version == &quot;7&quot; notify: restart service - name: service service: name=httpd state=started handlers: - name: restart service service: name=httpd state=restarted 迭代：with_items，类似于shell中的for循环迭代：当有需要重复性执行的任务时，可以使用迭代机制 对迭代项的引用，固定变量名为”item“ 要在task中使用with_items给定要迭代的元素列表 列表格式： 字符串 字典 字典构成一个键值对{key:vavul},如示例3 迭代的示例：示例1：比如创建user1.user2.user3个用户 - hosts: all remote_user: root tasks: - name: touch users user: name={{item}} with_items: - haha1 - haha2 - haha3 示例2：拷贝3个文件，file1 file2 file3 - hosts: all remote_user: root tasks: - name: copy files copy: src=/data/playbook/{{item}} dest=/data/ with_items: - file1 - file2 - file3 迭代嵌套子变量:涉及到多个键值对的表达方式示例3：创建3个组，再创建3个用户，指定加入一一对应的组 - hosts: all remote_user: root tasks: - name: creat groups group: name={{item}} with_items: - group1 - group2 - group3 - name: creat users user: name={{item.name}} group={{item.group}} with_items: - { name: &apos;haha1&apos;, group: &apos;group1&apos; } - { name: &apos;haha2&apos;, group: &apos;group2&apos; } - { name: &apos;haha3&apos;, group: &apos;group3&apos; } 备注：注意创建用户时，键值对的表达和使用方法 上面的执行结果是：先用单个迭代创建多个组，再通过多个键值对创建用户和组的一一对应关系:即：haha1属于group1;haha2属于group2;haha3属于group3; Playbook中template结合for循环生成具有重复性的代码段语法: for的写法： {% for vhost in nginx_vhosts %} server { listen {{ vhost.listen | default('80 default_server') }} ### Playbook中template结合for循环生成具有重复性的代码段 if的写法和表达的意思：如果键值对中的vhost.server_name被定义了，则使用 如果没定义，则不执行接下来的代码：示例2 {% if vhost.server_name is defined %} server_name {{ vhost.server_name }}; {% endif %} {% if vhost.root is defined %} root {{ vhost.root }}; {% endif %} ### for和if的示例，帮助理解其要执行语句的含义 示例2：生成listen加不同端口的和fqdn文件，由多个键值对组成 先创建for.j2文件： {% for i in ports %} server{ listen {{i.listen}} name {{i.name}} root {{i.root}} } {% endfor %} 创建playbook:再其中调用for.j2文件 - hosts: all remote_user: root vars: ports: - web1: listen: 81 name: www.baidu.com root: /data/web1 - web2: listen: 82 name: www.baidu1.com root: /data/web2 tasks: - name: test for template: src=for.j2 dest=/data/for1.conf 效果为： server{ listen 81 name www.baidu.com root /data/web1 } server{ listen 82 name www.baidu1.com root /data/web2 } 示例2：template配合if的涵义： 在示例1中的playbook中，把name注释掉，即不定义name的值 - web1: listen: 81 # name: www.baidu.com root: /data/web1 然后playbook:再调用for.j2文件 {% for i in ports %} server{ listen {{i.listen}} {% if i.name is defined%} 表示：如果i.name的值定义了，就用，没定义不用 name {{i.name}} {% endif %} root {{i.root}} } {% endfor %} 结果：则web1没有name的值，即可以理解if的用法 server{ listen 81 root /data/web1 少了web1的name的值 } server{ listen 82 name www.baidu1.com root /data/web2 } Roles：什么是roles,为什么要用roles？什么场景适合于roles?roles的结构？ansible重要内容之Roles；playbook的集合和拆分 ansilbe自1.2版本引入的新特性，用于层次性、结构化地组织playbook。roles 能够根据层次型结构自动装载变量文件、tasks以及handlers等。要使用roles只需 要在playbook中使用include指令即可。简单来讲，roles就是通过分别将变量、 文件、任务、模板及处理器放置于单独的目录中，并可以便捷地include它们的一 种机制。角色一般用于基于主机构建服务的场景中，但也可以是用于构建守护进程 等场景中 复杂场景：建议使用roles，代码复用度高 变更指定主机或主机组 如命名不规范维护和传承成本大 某些功能需多个Playbook，通过Includes即可实现 roles的意义和适用场景：角色(roles)：角色集合 适用场景：如系统内有多台数据库服务器，httpd服务器,nginx服务器，可以事先把 同一类的服务器所需的软件，数据库等写成各自的角色roles，然后就可以批量部署了， 当需要临时增加扩容一台服务器时，就可以使用事先编排好的role来对一台或多台服务器进行部署,从而提高了安装部署的效率。 如系统内会存在如下的各类服务，可以先编排好角色 roles/ ├── httpd/ ├── memcached/ ├── mysql/ └── nginx/ roles的目录结构（一般分成以下目录进行存放一类的文件）Roles各目录作用： /roles/project/ :项目名称,有以下子目录 如创建http，memcached，nginx等目录 files/ ：存放由copy或script模块等调用的文件 保存需要拷贝的配置文件 templates/：template模块查找所需要模板文件的目录 保存通过template的jinja2模板调用的配置文件 tasks/：定义task,role的基本元素，至少应该包含一个名为main.yml的文件； 其它的文件需要在此文件中通过include进行包含 handlers/：至少应该包含一个名为main.yml的文件；其它的文件需要在此 文件中通过include进行包含 vars/：定义变量，至少应该包含一个名为main.yml的文件；其它的文件需要 在此文件中通过include进行包含，可以单独定义变量的目录 meta/：定义当前角色的特殊设定及其依赖关系,至少应该包含一个名为 main.yml的文件，其它文件需在此文件中通过include进行包含 tasks目录下，组合任务顺序的文件 default/：设定默认变量时使用此目录中的main.yml文件 roles playbook的tags的标签的作用：通过标签可以更灵活的调用playbook的角色.- hosts: all remote_user: root roles: - {role: httpd,tags: [&apos;httpd&apos;,&apos;web&apos;]} - {role: memcached,tags: [&apos;memcached&apos;,&apos;web&apos;]} - {role: nginx,tags: [&apos;nginx&apos;,&apos;web1&apos;]} playbook调用角色：介绍前文提到的变量来源第六条：来自于roles的变量方法一：把需要调用的角色写在一个playbook里 - hosts: all remote_user: root roles: - role: httpd - role: memcached - role: nginx 弊端：如果要执行次playbook，三个角色都会执行一遍，不灵活 方法二；可以把变量在角色中定义 传递变量给角色 - hosts: remote_user: roles: - mysql - { role: nginx, username: nginx } 键role用于指定角色名称 后续的k/v用于传递变量给角色 调用角色方法3：还可基于条件测试实现角色调用 方法三：还可基于条件测试实现角色调用 roles: - { role: nginx, username: nginx, when: ansible_distribution_major_version == ‘7’ } roles示例：以httpd&amp;nginxmemcached三个role为例，下面为整个roles的目录结构和调用角色的playbook:role_playbook.ymlroles的目录结构下的httpd&amp;nginxmemcachedroles ├── httpd │ ├── files │ │ ├── index_6.html │ │ └── index_7.html │ ├── handlers │ │ └── main.yml │ ├── tasks │ │ ├── copyhtml_6.yml │ │ ├── copyhtml_7.yml │ │ ├── group.yml │ │ ├── main.yml │ │ ├── package.yml │ │ ├── service.yml │ │ ├── tempconfig_6.yml │ │ ├── tempconfig_7.yml │ │ └── user.yml │ ├── templates │ │ ├── httpd_6.conf.j2 │ │ └── httpd_7.conf.j2 │ └── vars ├── memcached │ ├── files │ ├── handlers │ │ └── main.yml │ ├── tasks │ │ ├── group.yml │ │ ├── main.yml │ │ ├── package.yml │ │ ├── service.yml │ │ ├── tempconfig.yml │ │ └── user.yml │ ├── templates │ │ └── memcached.j2 │ └── vars └── nginx ├── files │ ├── index_6.html │ └── index_7.html ├── handlers │ └── main.yml ├── tasks │ ├── copyhtml_6.yml │ ├── copyhtml_7.yml │ ├── group.yml │ ├── main.yml │ ├── package.yml │ ├── service.yml │ ├── tempconfig.yml │ └── user.yml ├── templates │ └── nginx.conf.j2 └── vars └── main.yml 调用角色的playbook:roles.yml可以通过加变量和标签和条件测试调用更灵活的调用各种角色) vim /data/roles.yml - hosts: all remote_user: root roles: - {role: httpd,tags: [&apos;httpd&apos;,&apos;web&apos;],when: ansible_distribution_major_version == &quot;6“} - {role: memcached,tags: [&apos;memcached&apos;,&apos;web&apos;]} - {role: nginx,tags: [&apos;nginx&apos;,&apos;web1&apos;]} 比如： 1.ansible-playbook -C -t httpd roles.yml 选择测试安装httpd，检查语法 2.ansible-playbook -t httpd roles.yml 只选择安装httpd 3.ansible-playbook -t nginx roles.yml 只选择安装nginx 4.ansible-playbook -t web roles.yml 安装httpd和memcached 5.ansible-playbook -t web1 roles.yml 只选择安装nginx 下图为每个role的各个文件内容：图一：参照roles的httpd的目录各个文件内容 图二：参照roles的nginx的目录各个文件内容 涉及到跨角色调用配置文件，避免产生多余的垃圾文件：截图上也有 跨角色调用配置文件写法： - name: copy index6 copy: src=roles/httpd/files/index_6.html dest=/usr/share/nginx/html/index.html 图三：参照roles的memcached的目录各个文件内容]]></content>
      <categories>
        <category>自动化运维</category>
      </categories>
      <tags>
        <tag>ansible,运维技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[httpd]]></title>
    <url>%2F2017%2F10%2F18%2Fhttpd%2F</url>
    <content type="text"><![CDATA[输入密码,PC:Enter查看,Phone:输入法换行查看. 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]]></content>
      <categories>
        <category>linux基础</category>
      </categories>
      <tags>
        <tag>文本三剑客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[文本三剑客之awk]]></title>
    <url>%2F2017%2F10%2F18%2F%E6%96%87%E6%9C%AC%E4%B8%89%E5%89%91%E5%AE%A2%E4%B9%8Bawk%2F</url>
    <content type="text"><![CDATA[文本处理三剑客之awk Awk的用户使用指南awk用户指南 相关链接文章：正则表达式： 正则表达式grep文本编辑： grep用法sed文本编辑： sed用法 总结对比一下这三个剑客的特长之处grep、sed、awk被称为linux中的三剑客 grep更适合单纯的查找或匹配文件.sed更适合编辑皮匹配到的文本awk更适合格式化文本，对文本进行比较复杂格式处理 文本三剑客都是默认逐行处理，自带循环sed 可以对文本进行修改，而grep&amp;awk 只是对文本进行过滤,不进行内容修改 awk中的-F后面的分隔符,要用单引号’’,双引号””会产生弱引用,比如特殊符号’\\‘ ‘\\$’关于awk中的单引号和双引号的问题参照：awk中的输入分隔符单引号&amp;双引号 学习awk的一个重要知识点 先举两个例子： awk &apos;/^UUID/&apos; /etc/fstab = awk &apos;/^UUID/{print $0}&apos; /etc/fstab 其实是隐藏了{print $0}的写法 数组中的例子 awk &apos;!line[$0]++&apos; f6 = awk &apos;!line[$0]++{print $0}&apos; f6 实际上是隐藏了{print $0} 学习中遇到的混淆的问题：&apos;!line[$0]++&apos;= &apos;!line[$0]++{print $0}&apos;是省略了写法，是patter的关系表达式中，先判断再打印 而后面的数组里的是加了双括号{}，即用的patter的空模式，即文本中的每一行都处理 Awk基本用法和功能以及各个功能示例：awk介绍awk基本用法awk变量awk格式化-printfawk操作符awk条件判断awk循环awk数组awk函数调用系统命令 awk介绍：whatis awk？ awk：Aho, Weinberger, Kernighan，报告生成器，格式化文本输出，输出成报表格式 linux上默认使用 GNU awk(gawk) [root@centos7 data]which awk /usr/bin/awk [root@centos7 data]ll /usr/bin/awk lrwxrwxrwx. 1 root root 4 Sep 19 11:45 /usr/bin/awk -&gt; gawk which awk=/usr/bin/awk 是gawk的软链接 awk基本用法awk [options] &apos;program&apos; var=value file… awk [options] -f programfile var=value file… awk [options] &apos;BEGIN{action;… }pattern{action;… }END{action;… }&apos; file ... awk 程序可由：BEGIN语句块、能够使用模式匹配的通用语句块、END语句 块，共3部分组成 program 通常是被放在单引号中 选项： -F “分隔符” 指明输入时用到的字段分隔符 -v var=value 变量赋值 基本格式：awk [options] &apos;program&apos; file… Program：pattern{action statements;..} 也就是说awk用法：awk [options] &apos;pattern{action statements;..}&apos; file… pattern和action • pattern部分决定动作语句何时触发及触发事件 BEGIN,END • action statements对数据进行处理，放在{}内指明 print, printf 分割符、域和记录 • awk执行时，由分隔符分隔的字段（域）标记$1,$2...$n称为域标识。$0 为所有域，注意：此时和shell中变量$符含义不同 • 文件的每一行称为记录 • 省略action，则默认执行 print $0 的操作 print格式：print item1, item2, ... 要点： (1) 逗号分隔符 (2) 输出item可以字符串，也可是数值；当前记录的字段、变量或awk的表达式 (3) 如省略item，相当于print $0 用法解析及示例：$0=代表处理的整行的内容 $1,$2,$3..代表每一列，也就域 BEGIN，END是为生成一个报表的头和尾准备的，用法通常为： BEGIN：为生成报告模式 添加表头;END:为生成的报告 进行信息汇总 awk &apos;BEGIN{print xxx}{print xxx}END{print xxx}&apos; 注意：BEGIN{print *xxx}处理文本前，打印一遍xxx的内容当成表头 END{print xxx},处理文本后，打印一遍xxx的内容作为表尾 BEGIN&amp;ENDBEGIN：让用户指定在第一条输入记录被处理之前所发生的动作，通常可在这里设置全局变量。 END：让用户在最后一条输入记录被读取之后发生的动作。 分隔符：awk默认使用空白符作为字段或列的分隔符；多个空白符被认为是一个，空白符包括空格，tab键/t，回车\n 也可以自定义-F&quot;分隔符&quot;自定义分隔符 print&amp;printf的区别：print命令只是单纯的把特定的内容进行打印，默认换行 printf命令可以把内容以灵活的格式进行显示,如左对齐,右对齐 示例1.awk支持标准输入输出，后面可以不跟文件 [root@centos7 ~]#awk &apos;{print $0}&apos; aaaa aaaa abcabc abcabc 2.打印/etc/passwd：对比几个输出结果 awk &apos;{print $0}&apos; /etc/passwd 把passwd文件全部打印出来 awk &apos;{print &quot;abc&quot;}&apos; /etc/passwd 读入的是passwd文件所有行，打印的是abc awk -v abc=1 &apos;{print abc}&apos; /etc/passwd 读入的是passwd文件所有行，打印的都是1 awk &apos;{print &quot;abc&quot;$0}&apos; /etc/passwd 把passwd文件所有行前都加一个abc,进行输出 所以在awk中不加双引号，abc被识别为变量，如果要引用变量，需要-v先定义值 如果只是想输出abc字符串，需要加双引号 3.awk{}中支持数字运算 awk &apos;{print 1+2}&apos; /etc/passwd 打印多行1+2=3的值 awk &apos;{print &quot;1+2&quot;}&apos; /etc/passwd 加双引号会把1+2当成字符串，输出多行1+2 4.取分区利用率df, df |awk &apos;{print $1,$5}&apos;| awk -F&quot;%&quot; &apos;{print $1}&apos; 5.以输入重定向方式，打印 passwd 文件的第1列，即获取当前系统中存在的所有用户和UID awk -F: &apos;{print $1,$3}&apos; &lt; /etc/passwd cat /etc/passwd | awk -F: &apos;{print $1,$3}&apos; awk -F: &apos;{print $1：$3}&apos; /etc/passwd 两列输出时，指定以：进行隔开，默认为空格隔开 awk -F: &apos;{print $1、$3}&apos; /etc/passwd 两列输出时，指定以：进行隔开，默认为空格隔开 cat /etc/passwd | awk -F: &apos;{print $1&quot;\n&quot; $3}&apos; 两列输出时,进行换行 cat /etc/passwd | awk -F: &apos;{print $1&quot;\t&quot; $3}&apos; 两列输出时,以tab键隔开 备注：多行输出时，可以在双引号之间加自定义的分隔符 格式：awk -F: &apos;{print $1&quot;=======&quot;$3}&apos; /etc/passwd Awk中的变量：变量分为：内置变量和自定义变量awk中的内置变量除了$0,$1,$2等，还有以下几种； 如果要使用这些变量需要加-v 选项先进行定义 FS：输入字段分隔符，默认为空白字符 =filed separator=域或列的分隔符 等于-F的选项，-F是选项，而FS是变量，实际作用是相等的 与-F的区别在于：可以下次调用FS变量 awk -v FS=&apos;:&apos; &apos;{print $1,$3}&apos; /etc/passwd = awk -F:&apos;{print $1,$3}&apos; /etc/passwd awk -v FS=&apos;:&apos; &apos;{print $1,FS$3}&apos; /etc/passwd 两列输出时以：做分隔符，调用变量FS awk -v FS=&apos;:&apos; &apos;{print $1,FS FS$3}&apos; /etc/passwd 两列输出时以：：做分隔符，调用2次变量FS 以空格隔开 可以先定义shell中的变量fs=:,awk再进行调用 fs=:;awk -v FS=$fs &apos;{print $1,FS,$3}&apos; /etc/passwd fs=:;awk –F$fs &apos;{print $1,$3,$7}&apos; /etc/passwd OFS：输出字段分隔符，默认为空白字符 =output filed separator 定义输出分隔符，不指定默认空空格做分隔符 awk -v FS=: -v OFS=+++ &apos;{print $1,$3,$7}&apos; /etc/passwd fs=+++;awk -v FS=: -v OFS=$fs &apos;{print $1,$3}&apos; /etc/passwd 调用shell变量做输出分隔符 RS：输入记录分隔符，指定输入时的换行符,自定义行的分隔符 =record记录 默认一行叫记录，行是以\n回车作为切割符的，RS可以自定义不用回车作为分隔符 awk -v RS=&apos; &apos; ‘{print }’ /etc/passwd awk -v RS=&apos;:&apos; &apos;{print NR,$0}&apos;/etc/passwd [root@centos7 ~]cat f1 aa;xxx:bb;bzzzz:cc dd:eex;zccc:xxxx [root@centos7 ~]cat f1| awk -v RS=&quot;:&quot; &apos;{print $0}&apos; aa;xxx bb;bzzzz cc dd eex;zccc xxxx 以RS=：冒号自定义行的分隔符，输出结果如上 [root@centos7 ~]#cat f1| awk -v FS=&quot;;&quot; -v RS=&quot;:&quot; &apos;{print $1}&apos; aa bb cc dd eex xxxx 自定义FS&amp;RS，输出结果如上 ORS：输出记录分隔符，输出时用指定符号代替换行符 awk -v RS=&apos; &apos; -v ORS=&apos;###&apos;‘{print }’ /etc/passwd [root@centos7 ~]cat f1 aa;xxx:bb;bzzzz:cc dd:eex;zccc:xxxx [root@centos7 ~]cat f1| awk -v FS=&quot;;&quot; -v RS=&quot;:&quot; -v ORS&quot;===&quot; &apos;{print $1}&apos; aa==bb==cc dd==eex==xxxx == 自定义FS,RS,ORS结果很明显 接下来是一个比较重要的变量 NF：字段数量,也就是域或列的总数量 awk -F: &apos;{print NF}&apos; /etc/passwd 以冒号做分隔符，显示每行的列的总数量 awk -F: &apos;{print $NF}&apos; /etc/passwd 显示以冒号做分隔符，每一行的最后一个字段即bash类型 awk -F：&apos;{print $(NF-1)}&apos; /etc/passwd 显示以冒号做分隔符，每一行的倒数第二个字段 统计光盘中所有安装包适用的cpu架构类型 root@centos7 mnt]#ls /mnt/Packages/*.rpm | awk -F&quot;.&quot; &apos;{print $(NF-1)}&apos;|sort|uniq -c 1371 noarch 2600 x86_64 NR：记录号，可以显示行数，如果有多个文件会合并后再统计总行 awk &apos;{print NR,$0}&apos; /etc/fstab 在每一行前加一个行号 awk BEGIN&apos;{print NR}&apos; /etc/fstab 输出结果为0 awk还没开始处理行，所以记录为0 awk END&apos;{print NR}&apos; /etc/fstab 输出结果为12 可以看出END是统计,awk处理的行数 1.通过加行号，可以很明显看出以冒号作为行的分隔符，每一行的都有什么；可以看出cc dd是一行的 [root@centos7 ~]cat f1 aa;xxx:bb;bzzzz:cc dd:eex;zccc:xxxx [root@centos7 ~]cat f1| awk -v RS=&quot;:&quot; &apos;{print NR$0}&apos; 1aa;xxx 2bb;bzzzz 3cc dd 4eex;zccc 5xxxx 2.备注：如果awk跟两个文件，awk会把两文件合并成一个文件，统计总行数，都取第一个字段的信息 如果需要分开显示统计，则用FNR [root@centos7 ~]awk -F: &apos;{print NR,$1}&apos; /etc/passwd /etc/group 1 root 2 bin 3 daemon 4 adm FNR：各文件分别计数,记录号 1.FNR:多个文件，每个分别统计显示第一个字段并列出来 awk &apos;{print FNR}&apos; /etc/fstab /etc/inittab [root@centos7 ~]awk -F: &apos;{print FNR,$1}&apos; /etc/passwd /etc/group 1 root 2 bin 3 daemon 48 quagga 49 httpd 1 root 2 bin FILENAME：当前文件名 1.统计时，加上变量可以显示文件名 awk &apos;{print FILENAME}&apos; /etc/fstab [root@centos7 ~]awk -F: &apos;{print FILENAME,FNR,$1}&apos; /etc/passwd /etc/passwd 1 root /etc/passwd 2 bin /etc/passwd 3 daemon ARGC：命令行参数的个数 awk &apos;{print ARGC}&apos; /etc/fstab /etc/inittab 结果为3 awk &apos;BEGIN {print ARGC}&apos; /etc/fstab /etc/inittab 结果为3 ARGC统计参数：awk单独算一个参数，后面的每个文件算一个参数,通过下面的数组可以体现出来 ARGV：数组，保存的是命令行所给定的各参数 1.显示awk的每个参数分别是哪个 [root@centos7 ~]awk &apos;{print ARGV[0]}&apos; /etc/fstab /etc/inittab awk [root@centos7 ~]awk &apos;{print ARGV[1]}&apos; /etc/fstab /etc/inittab /etc/fstab 示例：1.统计当前网络连接情况的ip地址是 ss -nt ss -nt | awk &apos;{print $5}&apos; 2.取/var/log/httpd/access_log的时间如下： root@centos7 ~]cat /var/log/httpd/access_log 192.168.34.1 - - [15/Oct/2018:22:02:35 +0800] &quot;GET / HTTP/1.1&quot; 403 4897 &quot;-&quot; 分两步取： cat /var/log/httpd/access_log | awk &apos;{print $4}&apos;|awk -F&quot;[&quot; &apos;{print $2}&apos; 一步取： cat /var/log/httpd/access_log | awk -F &quot;[\[ ]&quot; &apos;{print $5}&apos; 原理分析：[\[ ]代表（转义）\[和空格都算是分隔符，正则表达式的写法,或的关系， 而以空格分隔符，时间段为$4,那为什么是$5?在空格的标准里，[算$4,所以时间段为$5 3.取出磁盘分区利用率 -这次只取出利用率 两步取出： df | awk -F% &apos;{print $1}&apos;|awk &apos;{print $5}&apos; 一步取出： df | awk -F&quot;[[:space:]]+|%&quot; &apos;{print $5}&apos; awk用的是扩展的正则表达式 面试题：3-1,取出fstab中挂载的目录 [root@centos7 ~]cat /etc/fstab # See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info # UUID=9612633f-e7f1-4b28-8813-403d209d7abc / xfs defaults 0 0 UUID=0eba9e52-43a4-4c64-89bd-3fb639f0a6c1 /boot xfs defaults 0 0 UUID=06fe63d1-9b57-436f-ad7d-1c01c7a60aee /data xfs defaults 0 0 UUID=48e92f06-6fcb-4a21-8ba0-e7d1c5c1af39 swap swap defaults 0 0 [root@centos7 ~]cat /etc/fstab | awk -F&quot;[ ]/?&quot; &apos;/^UUID/{print $2}&apos; boot data swap 或者 [root@centos7 ~]cat /etc/fstab | awk -F&quot;[ ]/|[ ]&quot; &apos;/^UUID/{print $2}&apos; boot data swap 4.面试题：将文件f3中的第一个点前的字符串取出再写进去 [root@centos7 ~]cat f3 1 test.sina.com.cn 2 music.sina.com.cn 3 sports.sina.com.cn 4.news.sina.com.cn [root@centos7 ~]cat f3 | awk -F&quot;[ .]&quot; &apos;{print $2}&apos; &gt;&gt; f3 [root@centos7 ~]cat f3 1 test.sina.com.cn 2 music.sina.com.cn 3 sports.sina.com.cn 4.news.sina.com.cn test music sports news 4-1,扩展 前面-F&quot;[ .]&quot;既然是表达或的意思，那么是否可以这么写-F&quot;[ ]|.&quot;？？？ 答案：不可以！ 原因：因为此处用的是正则表达式，在正则表达式中点(.)代表任意单个字符，会把空格后的字母当成分隔符！ 所以是不可以的，那么如何写？ 如果不是点而是%呢？%是可以的，因为在正则表达式中%就是代表%， 但是$,或其他在正则表达式中有特殊含义的不可以作为分隔符，如果一定要做分隔符，需要反斜线转义 如下： 此处用3个反斜线转义 [root@centos7 ~]cat f2 | awk -F&quot;[ ]|\\\.&quot; &apos;{print $2}&apos; test music sports news 那如果文本中的第一个点是$呢？ 此处是4个反斜线进行转义 [root@centos7 ~]cat f2 1 test$sina.com.cn 2 music$sina.com.cn 3 sports$sina.com.cn 4 news$sina.com.cn [root@centos7 ~]cat f2 | awk -F&quot;[ ]|\\\\$&quot; &apos;{print $2}&apos; test music sports news [root@centos7 ~]cat f2 | awk -F&quot;[ $]&quot; &apos;{print $2}&apos; test music sports news 当用一个空格和具有特殊含义的符号时，最好是写在中括号[]里的 AWK中自定义变量自定义变量(区分字符大小写) (1) -v var=value (2) 在program中直接定义 (2-1)program可以放到一个文本里,awk -f 直接调用即可 示例：自定义变量可以-v var定义，也可以放到program即{}里，变量要先定义，后使用 awk -F: &apos;{name=&quot;magedu&quot;;print $1,name}&apos; /etc/passwd awk -F: -v name=&quot;magedu&quot; &apos;{print $1,name}&apos; /etc/passwd 例如 cat awk.txt {print $1,$2,$6} awk -F： -f awk.txt /etc/passwd =awk -F: &apos;{print $1,$2,$6}&apos; /etc/passwd Awk中的格式化在介绍printf前，先对其进行总结：1.使用printf动作输出的文本不会换行，如果需要换行，可以在对应的格式替换符后加入”\n”进行转义2.使用printf动作时，指定的格式和被格式化的文本之间，需要用”逗号”隔开。3.使用printf动作时，格式中的格式替换符比喻与被格式化的文本一一对应 printf命令-类似于shell里的printfprintf命令可以把内容以灵活的格式进行显示，如左对齐,右对齐 格式化输出：printf &quot;FORMAT&quot;, item1, item2, ... (1) 必须指定FORMAT (2) 不会自动换行，需要显式给出换行控制符，\n (3) FORMAT中需要分别为后面每个item指定格式符 格式符：与item一一对应 %c：显示字符的ASCII码 %d, %i：显示十进制整数 -用的比较多 %e, %E：显示科学计数法数值 %f：显示为浮点数 %g, %G：以科学计数法或浮点形式显示数值 %s：显示字符串 -用的比较多 %u：无符号整数 -用的比较多 %%：显示%自身 修饰符 #[.#] 第一个数字控制显示的宽度；第二个#表示小数点后精度，%3.1f + 左对齐（默认右对齐） %-15s * 显示数值的正负符号 %+d printf示例：1.设置对齐格式以及字符数 [root@centos7 ~]awk -F: &apos;{printf &quot;%-20s %-5d\n&quot;,$1,$3}&apos; /etc/passwd root 0 bin 1 pulse 171 gdm 42 gnome-initial-setup 990 $1为字符串，所以设置左对齐为20s个字符，$3为数字所以设置左对齐为5d个字符 printf默认不换行，所以需要加一个换行符 2.打印一个完整的报表格式 root@centos7 ~]awk -F: &apos;BEGIN{print &quot;username |uid\n--------&quot;} {printf &quot;%-10s |%-5d\n&quot;,$1,$3}END{print &quot;-------------&quot;}&apos; /etc/passwd username |uid ----------------------- root |0 bin |1 daemon |2 memcached |987 ceshi |1009 quagga |92 httpd |80 ------------------------- awk生成报表格式大概就是这个样子，所以awk称为报表生成器 3. [root@centos7 ~]awk -F: &apos;{printf &quot;Username: %s,UID:%d\n&quot;,$1,$3}&apos; /etc/passwd Username: root,UID:0 Username: bin,UID:1 Username: daemon,UID:2 Username: adm,UID:3 4. [root@centos7 ~]awk -F: &apos;{printf &quot;Username: %-15s,UID:%d\n&quot;,$1,$3}&apos; /etc/passwd Username: root ,UID:0 Username: bin ,UID:1 Username: daemon ,UID:2 awk操作符a.算术操作符： x+y, x-y, x*y, x/y, x^y, x%y - x：转换为负数 +x：将字符串转换为数值 字符串操作符：没有符号的操作符，字符串连接 赋值操作符： =, +=, -=, *=, /=, %=, ^=，++, --, b.比较操作符： ==, !=, &gt;, &gt;=, &lt;, &lt;= 模式匹配符： ~：左边是否和右边匹配包含 !~：是否不匹配 c.逻辑操作符：与&amp;&amp;，或||，非! d.条件表达式（三目表达式） selector?if-true-expression:if-false-expression 表达式；if-ture-xx:else-xxx eg:x&gt;y?var=x;var=y 操作符用法示例：1.下面两语句有何不同 • awk &apos;BEGIN{i=0;print ++i,i}&apos; 结果 1 1 • awk &apos;BEGIN{i=0;print i++,i}&apos; 结果 0 1 实际上AWK的语法是采用VC语言风格的 2.示例： awk中~&amp;!~是否包含的用法： [root@centos7 ~]awk -F: &apos;$0 ~ /root/{print $1}&apos; /etc/passwd root operator 意思是如果过滤的行中有包含root字符串的，则打印出这行的用户名 用到下文提到的patter模式，在这里是匹配是否包含root字符串 [root@centos7 ~]awk -F: &apos;$0 ~ &quot;/root&quot;{print $1}&apos; /etc/passwd root operator 区别上面的这个写法，在这里是包含/root字符串的行 [root@centos7 ~]awk -F: &apos;$0 !~ /root/{print $1}&apos; /etc/passwd bin daemon adm 和上面的命令刚好相反，如果行不包含字符串root，则打印该行用户名 [root@centos7 ~]awk -F: &apos;$3==0&apos; /etc/passwd root:x:0:0:root:/root:/bin/bash 意思：判断UID是否等于0，是则打印该行，判断是否为管理员 [root@centos7 ~]awk &apos;$0~&quot;^root&quot;&apos; /etc/passwd root:x:0:0:root:/root:/bin/bash 意思：判断该行是不是以root开头的行，是则打印 3.awk中的与&amp;&amp;，或|| 非!的使用示例： 示例： • awk –F: &apos;$3&gt;=0 &amp;&amp; $3&lt;=1000 {print $1}&apos; /etc/passwd 如果0&lt;=UID&lt;=1000，则打印出该用户 • awk -F: &apos;$3==0 || $3&gt;=1000 {print $1,$3}&apos; /etc/passwd 打印出UID等于0和UID&gt;=1000的用户名和他的UID • awk -F: &apos;!($3==0) {print $1}&apos; /etc/passwd -要加括号 打印出UID不等于0的用户名 • awk -F: &apos;!($3&gt;=500) {print $3}&apos; /etc/passwd 如果UID&lt;=500,时，打印出该用户的UID 4.AWK中的条件判断表达式 即三目表达式 相当于把shell中的if;then,else,fi的放到awk中 • 示例： [root@centos7 ~]awk -F: &apos;{$3&gt;=1000?name=&quot;common&quot;:name=&quot;sys&quot;;print name,$1,$3}&apos; /etc/passwd sys root 0 sys bin 1 sys tcpdump 72 common test 1000 common nginx 1008 判断用户是否为系统用户，是则打印并在开头加common,不是也打印在开头加sys awk中的PATTERN和action模式匹配和处理动作=sed的地址定界+修饰符功能：和sed中的pattern一样起到过滤的功能，=sed的地址定界 PATTERN:根据pattern条件，过滤匹配的行，再做处理(1)如果未指定：空模式，匹配每一行 (2) /regular expression/：仅处理能够模式匹配到的行，需要用/ /括起来 awk &apos;/^UUID/{print $1}&apos; /etc/fstab awk &apos;!/^UUID/{print $1}&apos; /etc/fstab awk的匹配模式支持的是扩展的正则表达式 注意：不支持直接给出数字格式，但是可以变向的打印输出，详见下面的示例 (3) relational expression: 关系表达式，结果为“真”才会被处理 真：结果为非0值，非空字符串 假：结果为空字符串或0值都是假 字符串为空或者0为假 (4) line ranges：行范围 startline,endline：/pat1/,/pat2/ 不支持直接给出数字格式 awk -F: &apos;/^root\&gt;/,/^nobody\&gt;/{print $1}&apos; /etc/passwd awk -F: &apos;(NR&gt;=10&amp;&amp;NR&lt;=20){print NR,$1}&apos; /etc/passwd NR表示行 (5) BEGIN/END模式 BEGIN{}: 仅在开始处理文件中的文本之前执行一次 END{}：仅在文本处理完成之后执行一次 模式：指定一个行的范围。该语法不能包括BEGIN和END模式。BEGIN：让用户指定在第一条输入记录被处理之前所发生的动作，通常可在这里设置全局变量。END：让用户在最后一条输入记录被读取之后发生的动作。 patter用法示例：先写一个特殊的用法 1.当在awk命令中使用正则模式时，使用到的正则用法属于&quot;扩展的正则表达式&quot; 2.当使用{x,y}这种次数匹配正则表达式，需要配合--posix或者--re-interval选项 备注：这是网上找到一个示例，文中提到{x,y}必须加这个选项，然而不加选项也是可以过滤的 awk是是支持posix字符集的 [root@centos7 ~]cat f3 seex sex seeex seeeeex [root@centos7 ~]cat f3 | awk &apos;/se{2,3}x/{print $0}&apos; seex seeex [root@centos7 ~]cat f3 | awk -posix &apos;/se{2,3}x/{print $0}&apos; seex seeex [root@centos7 ~]cat f3 | awk --re-interval &apos;/se{2,3}x/{print $0}&apos; seex seeex [root@centos7 ~]cat f3|grep -E &quot;se{2,3}x&quot; seex seeex [root@centos7 ~]cat f3|sed -nr &apos;/se{2,3}x/p&apos; seex seeex 1.取系统磁盘分区空间利用率df,/dev/sd开头的分区(上文中虽然取出来了，但是没过滤) [root@centos7 ~]#df | awk -F&quot;[[:space:]]+|%&quot; &apos;/^\/dev\/sd/{print $1,$5}&apos; /dev/sda2 8 /dev/sda3 1 /dev/sda1 17 2.取当前连接主机的IP地址(上文中虽然取出来了，但是没过滤) [root@centos7 ~]ss -nt| awk -F&quot;[[:space:]]+|:&quot; &apos;/^ESTAB/{print $6}&apos; 192.168.34.1 192.168.34.105 或者用NF的表达方式 [root@centos7 ~]ss -nt| awk -F&quot;[[:space:]]+|:&quot; &apos;/^ESTAB/{print $(NF-2)}&apos; 192.168.34.1 192.168.34.105 3.取登录当前系统失败（lastb）用户的IP [root@centos7 ~]#lastb root ssh:notty 192.168.34.105 Sun Nov 11 17:25 - 17:25 (00:00) root23 ssh:notty 192.168.34.1 Mon Nov 5 15:43 - 15:43 (00:00) btmp begins Fri Nov 2 09:58:52 2018 [root@centos7 ~]#lastb|awk &apos;$3 ~ /[[:digit:]]/{print $3}&apos;|sort|uniq -c 3 192.168.34.1 1 192.168.34.101 因为最后一行有btmp这样的字样，只能通过包含的功能来过滤掉最后一行 如果要取失败连接次数大于3的扔到防火墙，可以先取出来 root@centos7 ~]lastb|awk &apos;$3 ~ /[[:digit:]]/{print $3}&apos;|sort|uniq -c| awk &apos;$1 &gt;=3{print $2}&apos; 192.168.34.1 4.patter中为关系表达式的示例 空字符串或0值都是假，其他为真 awk &apos;0{print $0}&apos; /etc/passwd -0为假，结果为空 awk &apos;&quot;&quot;{print $0}&apos; /etc/passwd -空也为假，结果为空 awk &apos;1{print $0}&apos; /etc/passwd -1为真 awk &apos;&quot; &quot;{print $0}&apos; /etc/passwd -空白符也为真 awk -v abc=&quot; &quot; &apos;abc{print $0}&apos; /etc/passwd -abc为空白,不是空字符串,也为真 awk -v abc=&quot; &quot; &apos;! abc{print $0}&apos; /etc/passwd -abc为空白,为真,对真取反，结果为假 5.awk中patter的地址定界 root@centos7 ~]#awk -F: &apos;/^root/,/^adm/{print $0}&apos; /etc/passwd root:x:0:0:root:/root:/bin/bash bin:x:1:1:bin:/bin:/sbin/nologin daemon:x:2:2:daemon:/sbin:/sbin/nologin adm:x:3:4:adm:/var/adm:/sbin/nologin 打印以root开头的行到以adm开头的行之间的所有行 等于 sed -nr &apos;/^root/,/^adm/p&apos; /etc/passwd 6.如何打印从多少行到多少行之间的行？？ [root@centos7 ~]#awk -F: &apos;NR&gt;=10 &amp;&amp; NR&lt;=12 {print NR,$0}&apos; /etc/passwd 10 operator:x:11:0:operator:/root:/sbin/nologin 11 games:x:12:100:games:/usr/games:/sbin/nologin 12 ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin 通过变量NR变向的打印出行 7.取出/etc/fstab配置文件中以UUID开头的行 [root@centos7 ~]#cat /etc/fstab | awk &apos;/^UUID/{print $0}&apos; UUID=9612633f-e7f1-4b28-8813-403d209d7abc / xfs defaults 0 0 UUID=0eba9e52-43a4-4c64-89bd-3fb639f0a6c1 /boot xfs defaults 0 0 效果等于 grep &quot;^UUID&quot; /etc/fstab 效果等于 sed -n &apos;/^UUID/p&apos; /etc/fstab 8. awk &apos;!0&apos; /etc/passwd =awk &apos;!0{print $0}&apos; /etc/passwd 省略了{print $0}的写法 结果为真，即打印全部行 root@centos7 ~]#awk -F: &apos;i=1;j=1{print i,j}&apos; /etc/passwd root:x:0:0:root:/root:/bin/bash 1 1 bin:x:1:1:bin:/bin:/sbin/nologin 1 1 ？？？ [root@centos7 ~]#awk -F: &apos;$NF ~ /bash$/{print $1,$NF}&apos; /etc/passwd root /bin/bash test /bin/bash 判断用户shell是否为/bin/bash，是则打印用户名和shell类型，此处用变量NF来实现 效果等于 awk -F: &apos;$NF==&quot;/bin/bash&quot;{print $1,$NF}&apos; /etc/passwd 效果等于 awk -F: &apos;$7 == &quot;/bin/bash&quot;{print $1,$NF}&apos; /etc/passwd 效果等于 awk -F: &apos;$0 ~ /bash$/{print $1,$NF}&apos; /etc/passwd 9.打印奇数行和偶数行 [root@centos7 ~]#seq 6 | awk &apos;i=!i&apos; 打印奇数行 1 3 5 原理：i初始值为空，为假，取反时，则打印第一行，此时i=1 i=1时，为真，取反为假，所以第二行不打印，然后i=0 依次类推所以只打印奇数行 [root@centos7 ~]#seq 6 | awk &apos;!(i=!i)&apos; 打印偶数行 2 4 6 效果等于 seq 10 |awk -v i=1 &apos;i=!i&apos; 原理：同上，只要先定义i=1，为真，第一行就不打印了 或者用sed打印奇数行和偶数行(用sed步进的方式) seq 6 | sed -n &apos;1~2p&apos; 打印奇数行 seq 6 | sed -n &apos;2~2p&apos; 打印偶数行 awk actionaction除了上文中支持的算术、条件判断，表达式，还支持循环，输入语句，输出语句、组合语句等功能• (1) Expressions:算术，比较表达式等 • (2) Control statements：if, while等 • (3) Compound statements：组合语句 • (4) input statements • (5) output statements：print等 下面专门研究awk的控制语句包含if,while,do,for,break,continue,数组，exit等控制语句awk中if-else控制语句的语法及用法语法： 双分支if if(condition){statement;…}(多条语句用;隔开)[else statement] 多分支if if(condition1){statement1}else if(condition2){statement2}else{statement3} 使用场景：对awk取得的整行或某个字段做条件判断 if-else示例：如判断考试分数，写法如下 [root@centos7 ~]awk -v score==80 &apos;BEGIN{if(score &lt;60){print &quot;no pass&quot;} else if(score &lt;=80){print &quot;soso&quot;}else {print good}}&apos; soso awk -F: &apos;{if($3&gt;=1000)print $1,$3}&apos; /etc/passwd 判断UID是否大于1000，是则打印用户名和UID awk -F: &apos;{if($NF==&quot;/bin/bash&quot;) print $1}&apos; /etc/passwd 判断用户shell是否为/bin/bash,是则打印用户名 awk &apos;{if(NF&gt;5) print $0}&apos; /etc/fstab 判断域或列个数是否大于5，是则打印该行 awk -F: &apos;{if($3&gt;=1000) {printf &quot;Common user: %s\n&quot;,$1} else {printf &quot;root or Sysuser: %s\n&quot;,$1}}&apos; /etc/passwd 等于上文中的 awk -F: &apos;{$3&gt;=1000?name=&quot;common&quot;:name=&quot;sys&quot;;print name,$1,$3}&apos; /etc/passwd 例如：随机生成1000个数字，用awk取出最大值和最小值(可以用shell写法) 1.生成1000个数字 for i in {1..1000};do if [ $i -eq 1 ];then echo -e &quot;$RANDOM\c&quot; &gt;&gt; f1.txt;else echo -e &quot;,$RANDOM\c&quot; &gt;&gt; f1.txt ;fi;done 2.用awk取出最大值和最小值 awk -F&apos;,&apos; &apos;{i=2;max=$1,while(i&lt;=NF){if($i &gt; max){max=$i}else if($i &lt; min){min=$i};i++}}END{print &quot;max=&quot;max,&quot;min=&quot;min}&apos; f1.txt 验证用awk是否取出的值为正确的： 方法一：用tr验证 tr &apos;,&apos; &apos;\n&apos; &lt; f1.txt | sort -nr | head -n1 tr &apos;,&apos; &apos;\n&apos; &lt; f1.txt | sort -nr | tail -n1 方法二：用shell脚本验证 #!/bin/bash for i in {1..1000};do awk中的循环不是对行的循环，因为awk本身就支持文本中行的循环，这里是指对每一行中的字段或域或列进行循环，分别对行中的每个字段进行循环处理awk中while循环控制语句的语法及用法语法：while(condition){statement;…} 条件“真”，进入循环；条件“假”，退出循环 使用场景： 对一行内的多个字段逐一类似处理时使用 对数组中的各元素逐一处理时使用 此时涉及到系统自带的一个函数length(函数在下面会有介绍) 示例： 1.统计每一行第一个字段的长度 root@centos7 ~]awk -F: &apos;{print length($1),$1}&apos; /etc/passwd 4 root 3 bin 6 daemon 3 adm 2.统计/etc/passwd第一行的每个字段的长度 [root@centos7 ~]awk -F: &apos;NR==1{i=1;while(i&lt;=NF){print $i,length($i);i++}}&apos; /etc/passwd root 4 x 1 0 1 0 1 root 4 /root 5 /bin/bash 9 3.统计grub2.cfg文件中linux16那行的每个字段的长度 [root@centos7 ~]awk &apos;/^[[:space:]]*linux16/{i=1;while(i&lt;=NF){print $i,length($i);i++}}&apos; /etc/grub2.cfg linux16 7 /vmlinuz-3.10.0-862.el7.x86_64 30 root=UUID=9612633f-e7f1-4b28-8813-403d209d7abc 46 ro 2 crashkernel=auto 16 rhgb 4 quiet 5 LANG=en_US.UTF-8 16 linux16 7 /vmlinuz-0-rescue-d874c9913a8e4f4f8b615f29c9a0388e 50 root=UUID=9612633f-e7f1-4b28-8813-403d209d7abc 46 ro 2 crashkernel=auto 16 rhgb 4 quiet 5 4.统计grub2.cfg文件中linux16那行的字段长度大于10的字段-用while循环 root@centos7 ~]awk &apos;/^[[:space:]]*linux16/{i=1;while(i&lt;=NF){if(length($i)&gt;=10) {print $i,length($i)};i++}}&apos; /etc/grub2.cfg /vmlinuz-3.10.0-862.el7.x86_64 30 root=UUID=9612633f-e7f1-4b28-8813-403d209d7abc 46 crashkernel=auto 16 LANG=en_US.UTF-8 16 /vmlinuz-0-rescue-d874c9913a8e4f4f8b615f29c9a0388e 50 root=UUID=9612633f-e7f1-4b28-8813-403d209d7abc 46 crashkernel=auto 16 5.面试题 用awk取出文本f5的最大值和最小值，需要先生成1000个随机数存到f5中 如何做？ 1.不用awk，可以通过脚本实现最大值和最小值 2.用awk如何来做？？ 先生成1000个随机数 [root@centos7 ~]for i in `seq 1000`;do if [ $i -eq 1 ];then echo -e &quot;$RANDOM\c&quot; &gt;&gt; f5; else echo -e &quot;,$aRANDOM\c&quot; &gt;&gt; f5 ;fi;done 生成了1000随机数，如何取最大值最小值？ [root@centos7 ~]awk -F&quot;,&quot; &apos;{i=2;max=$1;min=$1;while(i&lt;=NF){if($i &gt; max){max=$i} else if($i &lt; min){min=$i};i++}}END{print &quot;max=&quot;max,&quot;min=&quot;min}&apos; &lt; f5 max=32643 min=60 awk中do-while循环控制语句语法：do {statement;…}while(condition) 意义：无论真假，至少执行一次循环体 do-while使用示例：求1-100正整数的和 [root@centos7 ~]awk &apos;BEGIN{ total=0;i=0;do{ total+=i;i++;}while(i&lt;=100);print total}&apos; 5050 awk中for循环控制语句语法：for(expr1;expr2;expr3) {statement;…} 常见用法： for(variable assignment;condition;iteration process) {for-body} 特殊用法：能够遍历数组中的元素 语法：for(var in array) {for-body} for循环使用示例：1.求1-100正整数的和： [root@centos7 ~]awk &apos;BEGIN{for(i=1;i&lt;=100;i++)sum+=i;print sum}&apos; 5050 2.统计grub2.cfg文件中linux16那行的字段长度大于10的字段-用for循环 [root@centos7 ~]awk &apos;/^[[:space:]]*linux16/{for(i=1;i&lt;=NF;i++){if(length($i)&gt;=10){print $i,length($i)}}}&apos; /etc/grub2.cfg /vmlinuz-3.10.0-862.el7.x86_64 30 root=UUID=9612633f-e7f1-4b28-8813-403d209d7abc 46 crashkernel=auto 16 LANG=en_US.UTF-8 16 /vmlinuz-0-rescue-d874c9913a8e4f4f8b615f29c9a0388e 50 root=UUID=9612633f-e7f1-4b28-8813-403d209d7abc 46 crashkernel=auto 16 awk中的switch控制语句类似于shell中的case语句 语法：switch(expression) {case VALUE1 or /REGEXP/: statement1; caseVALUE2 or /REGEXP2/: statement2; ...; default: statementn} awk中的continue,break，next控制语句break和continuenext:提前结束对本行处理而直接进入下一行处理（awk自身循环） continue的示例求1000以内偶数的和 [root@centos7 ~]awk &apos;BEGIN{sum=0;for(i=1;i&lt;=100;i++){if(i%2==0)continue;sum+=i}print sum}&apos; 2500 求1000以内奇数的和 [root@centos7 ~]awk &apos;BEGIN{sum=0;for(i=1;i&lt;=100;i++){if(i%2==1)continue;sum+=i}print sum}&apos; 2550 求1000以内除了66的所有数字的和 [root@centos7 ~]awk &apos;BEGIN{sum=0;for(i=1;i&lt;=100;i++){if(i==66)continue;sum+=i}print sum}&apos; 4984 break的示例：求1000数字中，当大于100时，跳出循环，即求100以内的和 [root@centos7 ~]#awk &apos;BEGIN{sum=0;for(i=1;i&lt;=1000;i++){if(i&gt;100)break;sum+=i}print sum}&apos; 5050 next的示例：因为提前结束对本行处理而直接进入下一行处理（因为awk本身就是循环行的功能），所以示例 打印/etc/passwd下的奇数行 [root@centos7 ~]awk -F: &apos;{if(NR%2==0)next;print NR,$0}&apos; /etc/passwd 1 root:x:0:0:root:/root:/bin/bash 3 daemon:x:2:2:daemon:/sbin:/sbin/nologin 5 lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin 7 shutdown:x:6:0:shutdown:/sbin:/sbin/shutdown 9 mail:x:8:12:mail:/var/spool/mail:/sbin/nologin 11 games:x:12:100:games:/usr/games:/sbin/nologin 13 nobody:x:99:99:Nobody:/:/sbin/nologin 还可以这样写：awk -F: &apos;NR%2==1{print NR,$0}&apos; /etc/passwd awk数组-一个非常使用的功能awk的数组全都是关联数组关联数组：array[index-expression] index-expression: • (1) 可使用任意字符串；字符串要使用双引号括起来 • (2) 如果某数组元素事先不存在，在引用时，awk会自动创建此元素，并将其值 初始化为“空串” • (3) 若要判断数组中是否存在某元素，要使用“index in array”格式进行遍历 数组的去重的效果示例：awk &apos;!arr[$0]++&apos; dupfile awk &apos;{!arr[$0]++;print $0, arr[$0]}&apos; dupfile echo abc abc ccc bcd ddd ddd abc abc &gt;&gt; f1.txt awk关联数组的遍历：若要遍历数组中的每个元素，要使用for循环:因为关联数组下标是无序性 for(var in array) {for-body} 注意：var会遍历array的每个索引 数组的使用示例：示例1.定义awk数组 [root@centos7 ~]awk &apos;BEGIN{title[&quot;ceo&quot;]=&quot;zhang&quot;;title[&quot;coo&quot;]=&quot;liu&quot;; title[&quot;cto&quot;]=&quot;wang&quot;;print title[&quot;ceo&quot;]}&apos; zhang 可以用for循环把每一个数组的值都表示出来 [root@centos7 ~]awk &apos;BEGIN{title[&quot;ceo&quot;]=&quot;zhang&quot;;title[&quot;coo&quot;]=&quot;liu&quot;;title[&quot;cto&quot;]=&quot;wang&quot;;print title[&quot;ceo&quot;]; for(i in title){print i,title[i]}}&apos; zhang coo liu ceo zhang cto wang 但输出时数组元素时，是无序的，这就是关联数组的特性 示例2.awk数组中的重要功能 [root@centos7 ~]cat f6 abc abc ddd ccc aaa ccc ccc [root@centos7 ~]awk &apos;!line[$0]++&apos; f6 abc ddd ccc aaa 问题：为什么执行结果是这个？？ 原因： awk &apos;!line[$0]++&apos; f6 = awk &apos;!line[$0]++{print $0}&apos; f6 实际上是隐藏了{print $0}，这个隐藏功能在awk很重要！！！， 但是要和后面patter中的空模式区别开 &apos;!line[$0]++&apos;= &apos;!line[$0]++{print $0}&apos;是省略了写法，是patter的关系表达式中，先判断再打印 而后面的数组里的是加了双括号{}，{line[$0]++}即用的patter的空模式，即文本中的每一行都处理 这两个写法是不一样的，别混淆了 分析： 基于这个考虑进行分析 &apos;!line[$0]++&apos; = &apos;!line[$0]++{print $0}&apos; 当读取文本f6的第一行时=abc !line[&quot;abc&quot;]++，因为abc没有值，line[&quot;abc&quot;]=0为假，取反！line[&quot;abc&quot;]后为真 所以把第一行的abc打印了一次，打印第一行结束后line[&quot;abc&quot;]=1 当读取文本f6的第二行时=abc !line[&quot;abc&quot;]++，在之前基础上line[&quot;abc&quot;]=1为假，取反！line[&quot;abc&quot;]=0后为假 所以把第二行的abc不打印，结束后line[&quot;abc&quot;]=2 以此类推，发现后续出现abc都不会打印，但是line[&quot;abc&quot;]都会在之前的基础上+1，即达到去重并统计的目的 而处理第三行!line[&quot;ddd&quot;]++时，和上述abc一样，第一次出现则打印，后续也不会打印 所以，命令执行结果为去重的效果 从这个命令执行结果也可以明显看到上述的分析结果 可以看出abc的值是递增的，也就是abc出现的次数 [root@centos7 ~]awk &apos;{!line[$0]++;print $0, line[$0]}&apos; f6 abc 1 abc 2 ddd 1 ccc 1 aaa 1 ccc 2 awk数组中的重要功能之for循环遍历数组，很具有实用性在后面的统计服务的一些日志文件很有作用如果要理解遍历awk数组，需要深刻理解上述的示例2:awk &apos;!line[$0]++&apos; f6是怎么实现的 但还是有些和数组里的，遍历$2,把$2不同的值，当成数组的下标有些区别的 若要遍历数组中的每个元素，要使用for循环for(var in array) {for-body} 注意：var会遍历array的每个索引 为什么要通过特殊写法去遍历awk中的数组？如果是普通数组，用循环0,1,2,3做个循环即可；而awk的数组都是关联数组，[]中括号里的下标都不固定 所以要通过 for(var in array(数组名称)) {for-body}这样的特殊写法，var的值会从array数组元素 取其下标，相当于每次循环var的值是等于array数组的下标的 注意：当我们直接引用一个数组中不存在的元素时，awk会自动创建这个元素，并且为其赋值为&quot;空字符串&quot; for循环遍历数组使用示例：示例1： [root@centos7 ~]awk &apos;BEGIN{print test[&quot;ip&quot;];test[&quot;ip&quot;]++;print test[&quot;ip&quot;]}&apos; 1 第一次输出为空，第二次自动加1 示例2： 1. [root@centos7 ~]awk &apos;BEGIN{title[&quot;ceo&quot;]=&quot;zhang&quot;;title[&quot;coo&quot;]=&quot;liu&quot;;title[&quot;cto&quot;]=&quot;wang&quot;;for(i in title){print title[i]}}&apos; liu zhang wang 分析： for(i in title){print title[i]，i的值来自于title的下标，即i=ceo coo cto,而且是无序打印 示例3： [root@centos7 ~]netstat -tan Active Internet connections (servers and established) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:111 0.0.0.0:* LISTEN tcp 0 0 192.168.122.1:53 0.0.0.0:* ESTABLISTEN tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN tcp 0 0 127.0.0.1:631 0.0.0.0:* LISTEN tcp 0 0 127.0.0.1:25 0.0.0.0:* LISTEN tcp 0 52 192.168.34.103:22 192.168.34.1:8816 ESTABLISHED tcp 0 0 192.168.34.103:22 192.168.34.105:49746 ESTABLISHED tcp6 0 0 :::111 :::* LISTEN tcp6 0 0 :::22 :::* LISTEN tcp6 0 0 ::1:631 :::* LISTEN tcp6 0 0 ::1:25 :::* LISTEN [root@centos7 ~]netstat -tan | awk &apos;/^tcp/{state[$NF]++}END{for(i in state) { print i,state[i]}}&apos; LISTEN 8 ESTABLISHED 3 分析： state[$NF]++以空白做分隔符，统计同一类型的状态有多少个 for(i in state) { print i,state[i]} 即统计了出现状态的多少种以及出现的次数 不明白的可以看上文中的示例2：awk数组中的重要功能 当然，看懂这个命令，需要知道两个知识点 1.空模式，即全文反复提到的{state[$NF]++}={state[$NF]++}{print $0} 2.END模式，END的模式是在前面动作结束之后，最后执行依次END模式中的命令，所以先不用考虑空模式 下面再一次对空模式中的处理过程，做详细的描述 空模式中，我们创建了一个state数组，并将状态值(LISTEN、ESTABLISHED)作为引用下标， 所以执行到第一行时，我们引用的是state[&quot;LISTEN&quot;],很明显，这个元素并不存在，所以，当第一行被空模式的动作处理完毕后 state[&quot;LISTEN&quot;]的值已经被赋值为1了。 这时，空模式中的动作继续处理下一行，而下一行的$NF=ESTABLISTEN；state[&quot;ESTABLISTEN&quot;] 所以，state[&quot;ESTABLISTEN&quot;]第一次参与运算过程与上一个原理相同 直到再次遇到相同的状态时，使用同样一个IP地址作为下标的元素将会再次被自加 直到处理完所有的行，开始执行END模式中的动作。 而END模式中，我们打印出state数组中的所有元素的下标，以及元素对应的值。 此刻，state数组中的下标即为各种状态，元素的值即为对应状态出现的次数， 最终，我们统计出每个状态出现的次数。 3.统计/var/log/httpd/access_log，每个IP链接的次数 root@centos7 ~]cat /var/log/httpd/access_log 192.168.34.1 - - [15/Oct/2018:22:02:35 +0800] &quot;GET / HTTP/1.1&quot; 403 4897 &quot;-&quot; &quot;Mozil&quot; 192.168.34.1 - - [15/Oct/2018:22:02:35 +0800] &quot;GET /noindex/css/bootstrap.min.css&quot; 192.168.34.103 - - [15/Oct/2018:22:02:35 +0800] &quot;GET /noindex/css/bootstrap.min.css&quot; 192.168.34.1 - - [15/Oct/2018:22:02:35 +0800] &quot;GET /noindex/css/bootstrap.min.css&quot; [root@centos7 ~]awk &apos;{ip[$1]++}END{for(i in ip){print i,ip[i]}}&apos; /var/log/httpd/access_log 192.168.34.101 9 192.168.34.103 13 ::1 4 192.168.34.1 88 效果等于： [root@centos7 ~]awk &apos;{print $1}&apos; /var/log/httpd/access_log |sort|uniq -c 4 ::1 88 192.168.34.1 9 192.168.34.101 13 192.168.34.103 4.统计ss -nt ip链接次数 [root@centos7 ~]ss -nt State Recv-Q Send-Q Local Address:Port Peer Address:Port ESTAB 0 52 192.168.34.103:22 192.168.34.1:8816 ESTAB 0 0 192.168.34.103:22 192.168.34.105:49746 [root@centos7 ~]ss -nt|awk -F&quot;[[:space:]]+|:&quot; &apos;/^ESTAB/{ip[$(NF-2)]++}END{for(i in ip){print i,ip[i]}}&apos; 192.168.34.105 1 192.168.34.1 1 效果等于： [root@centos7 ~]ss -nt|awk -F&quot;[[:space:]]+|:&quot; &apos;/^ESTAB/{print $(NF-2)}&apos;|sort|uniq -c 1 192.168.34.1 1 192.168.34.105 5.统计/etc/fstab文件系统类型分别有多少个 [root@centos7 ~]cat /etc/fstab # /etc/fstab # Created by anaconda on Wed Sep 19 11:44:48 2018 UUID=9612633f-e7f1-4b28-8813-403d209d7abc / xfs defaults 0 0 UUID=0eba9e52-43a4-4c64-89bd-3fb639f0a6c1 /boot xfs defaults 0 0 UUID=06fe63d1-9b57-436f-ad7d-1c01c7a60aee /data xfs defaults 0 0 UUID=48e92f06-6fcb-4a21-8ba0-e7d1c5c1af39 swap swap defaults 0 0 [root@centos7 ~]cat /etc/fstab | awk &apos;/^UUID/{file[$3]++}END{for(i in file){print i,file[i]}}&apos; swap 1 xfs 3 6.求下表中的男生和女生的平均成绩 [root@centos7 ~]cat f9 name sex score a m 90 b f 80 c f 99 d m 88 e m 80 如何利用awk的数组功能来求？ 思路先求男的和和女的和？ 利用两个数组？ [root@centos7 ~]cat f9|awk &apos;!/^name/{sum[$2]+=$3}END{for(i in sum){print i,sum[i]}}&apos; -求男女分的和 m 258 f 179 [root@centos7 ~]cat f9|awk &apos;!/^name/{sum[$2]+=$3;num[$2]++}END{for(i in sum){print i,sum[i]/num[i]}}&apos; m 86 f 89.5 7.统计下面每个名字出现的次数 [root@centos7 ~]cat f1 Allen Phillips Green Lee William Aiden Janmes Lee Angel Jack Jack Thomas Lucas Kevin Tyler Lee William Allen [root@centos7 ~]cat f1 | awk &apos;{for(i=1;i&lt;=NF;i++){count[$i]++}}END{for(j in count){print j,cuont[j]}}&apos; Tyler Angel Lucas William Thomas Green Jack Phillips Kevin awk函数awk也包括内置函数和自定义函数内置函数包括 rand,length，sub,gsub,split，system数值处理： rand(i)：返回0和1之间一个随机数 awk &apos;BEGIN{srand(); for (i=1;i&lt;=10;i++)print int(rand()*100) }&apos; 字符串处理： • length([s])：返回指定字符串的长度 • sub(r,s,[t])：对t字符串搜索r表示模式匹配的内容，并将第一个匹配内容替换为s echo &quot;2008:08:08 08:08:08&quot; | awk &apos;sub(/:/,“-&quot;,$1)&apos; • gsub(r,s,[t])：对t字符串进行搜索r表示的模式匹配的内容，并全部替换为s所表 示的内容 echo &quot;2008:08:08 08:08:08&quot; | awk &apos;gsub(/:/,&quot;-&quot;,$0)&apos; • split(s,array,[r])：以r为分隔符，切割字符串s，并将切割后的结果保存至array所 表示的数组中，第一个索引值为1,第二个索引值为2,… netstat -tn | awk &apos;/^tcp\&gt;/{split($5,ip,&quot;:&quot;);count[ip[1]]++}&apos; END{for (i in count) {print i,count[i]} awk内置函数之sub、gsub、split实现搜索替换切割的用法示例1： sub(r,s,[t])：对t字符串搜索r表示模式匹配的内容，并将第一个匹配内容替换为s gsub(r,s,[t])表示全局替换 sub(r,s,[t])不是贪婪模式，默认值替换搜索出来的第一个 [root@centos7 ~]#echo &quot;2008:08:08 08:08:08&quot; | awk &apos;sub(/:/,&quot;-&quot;,$1)&apos; 2008-08:08 08:08:08 只替换$1 root@centos7 ~]#echo &quot;2008:08:08 08:08:08&quot; | awk &apos;gsub(/:/,&quot;-&quot;,$1)&apos; 2008-08-08 08:08:08 全局替换$0 [root@centos7 ~]#echo &quot;2008:08:08 08:08:08&quot; | awk &apos;gsub(/:/,&quot;-&quot;,$0)&apos; 2008-08-08 08-08-08 示例2： 统计字符串中每个字母出现的次数abcdaabbccdd [root@centos7 mnt]#echo abcdaabbccdd | awk &apos;gsub(//,&quot;.&quot;,$0)&apos;|awk -F&quot;.&quot; &apos;{for(i=2;i&lt;=NF-1;i++)num[$i]++}END{for(j in num){print j,num[j]}}&apos; a 3 b 3 c 3 d 3 在文章最后会用另外一种写法进行过滤(文章最后会以以空位分隔符的特殊写法来表示) awk内置函数split的切割功能示例1: 统计链接本机的IP和端口号 [root@centos7 ~]#netstat -tn Active Internet connections (w/o servers) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 52 192.168.34.103:22 192.168.34.1:8816 ESTABLISHED tcp 0 0 192.168.34.103:22 192.168.34.105:49746 ESTABLISHED [root@centos7 ~]netstat -tn | awk &apos;/^tcp\&gt;/{split($5,ip,&quot;:&quot;);count[ip[1]]++}END{for (i in count) {print i,count[i]}}&apos; 192.168.34.105 1 192.168.34.1 1 [root@centos7 ~]netstat -tn | awk &apos;/^tcp\&gt;/{split($5,ip,&quot;:&quot;);count[ip[2]]++}END{for (i in count) {print i,count[i]}}&apos; 8816 1 49746 1 分析： split($5,ip,&quot;:&quot;)将192.168.34.1:8816进行切割，数组ip的下标1放IP地址，下标2放端口号 count[ip[1]]++ 把ip下标为1也就是IP地址，把出现的IP个数，又给count当下标，就统计每个IP出现的次数 count[ip[2]]++ 把ip下标为2也就是端口号，把出现的端口号个数，又给count当下标，就统计每个端口号出现的次数 awk中的自定义函数格式：awk自定义函数是用正规开发语言的函数格式 function name ( parameter, parameter, ... ) { statements return expression } awk自定义函数的用法：cat fun.awk,把函数写到文件中 function max(x,y) { x&gt;y?var=x:var=y return var } BEGIN{print max(i,j)} awk -v i=10 -v j=20 -f fun.awk 调用函数即可，和shell中的函数类似 awk中很实用的内置函数system命令system函数作用：在awk可以反过来调用linux里的命令示例： 空格是awk中的字符串连接符，如果system中需要使用awk中的变量可以使用 空格分隔，或者说除了awk的变量外其他一律用&quot;&quot;引用起来 示例1: 显示/boot/grub2下的文件列表；调用命令时要加双引号 [root@centos7 ~]#awk &apos;BEGIN{system(&quot;ls /boot/grub2&quot;)}&apos; device.map fonts grub.cfg grubenv i386-pc locale 或者这么写，先定义变量等于路径，再调用变量 [root@centos7 ~]#awk &apos;BEGIN{dir=&quot;/boot/grub2&quot;;system(&quot;ls &quot;dir)}&apos; device.map fonts grub.cfg grubenv i386-pc locale 调用hostname命令，显示主机名 [root@centos7 ~]#awk &apos;BEGIN{system(&quot;hostname&quot;)}&apos; centos7.localdomain 示例2： 之前处理过lastb中登录失败的IP，如果失败登录次数大于3次，扔到防火墙里， 当时是先取出IP放到文件里，然后iptables再禁用； 现在可以在awk中先取出IP,然后通过system(&quot;iptables &quot; IP)就可以直接扔到防火墙中 具体实现？ awk脚本将awk程序写成脚本，直接调用或执行 awk脚本使用示例： 1.先写文本再调用 cat f1.awk {if($3&gt;=1000)print $1,$3} awk -F: -f f1.awk /etc/passwd 2.也可以写成脚本形式 先写再调用 [root@centos7 ~]vim f2.awk #!/bin/awk -f {if($3 &gt;=1000)print $1,$3} [root@centos7 ~]./f2.awk -F: /etc/passwd nfsnobody 65534 test 1000 gentoo 1007 nginx 1008 ceshi 1009 向awk脚本传递参数格式： awkfile var=value var2=value2... Inputfile 注意：在BEGIN过程中不可用。直到首行输入完成以后，变量才可用。可以通 过-v 参数，让awk在执行BEGIN之前得到变量的值。命令行中每一个指定的变 量都需要一个-v参数 awk脚本传参使用示例：cat test.awk #!/bin/awk –f {if($3 &gt;=min &amp;&amp; $3&lt;=max)print $1,$3} chmod +x test.awk test.awk -F: min=100 max=200 /etc/passwd 工作中遇到的常用awk文本解决案例：Linux Web服务器网站故障分析常用的命令系统连接状态篇： 1.查看TCP连接状态 netstat -nat |awk &apos;{print $6}&apos;|sort|uniq -c|sort -rn 每出现一被/^tcp/模式匹配到的行，数组S[$NF]就加1，NF为当前匹配到的行的最后一个字段，此处用其值做为数组S的元素索引； netstat -n | awk &apos;/^tcp/ {++S[$NF]};END {for(a in S) print a, S[a]}&apos; 或 netstat -n | awk &apos;/^tcp/ {++state[$NF]}; END {for(key in state) print key,&quot;\t&quot;,state[key]}&apos; netstat -n | awk &apos;/^tcp/ {++arr[$NF]};END {for(k in arr) print k,&quot;t&quot;,arr[k]}&apos; netstat -n |awk &apos;/^tcp/ {print $NF}&apos;|sort|uniq -c|sort -rn netstat -ant | awk &apos;{print $NF}&apos; | grep -v &apos;[a-z]&apos; | sort | uniq -c 2.查找请求数请20个IP（常用于查找攻来源）： 方法一： netstat -anlp|grep 80|grep tcp|awk ‘{print $5}’|awk -F: ‘{print $1}’|sort|uniq -c|sort -nr|head -n20 方法二： netstat -ant |awk &apos;/:80/{split($5,ip,&quot;:&quot;);++A[ip[1]]}END{for(i in A) print A[i],i}&apos; |sort -rn|head -n20 3.用tcpdump嗅探80端口的访问看看谁最高 tcpdump -i eth0 -tnn dst port 80 -c 1000 | awk -F&quot;.&quot; ‘{print $1&quot;.&quot;$2&quot;.&quot;$3&quot;.&quot;$4}’ | sort | uniq -c | sort -nr |head -20 4.查找较多time_wait连接 netstat -n|grep TIME_WAIT|awk ‘{print $5}’|sort|uniq -c|sort -rn|head -n20 5.找查较多的SYN连接 netstat -an | grep SYN | awk ‘{print $5}’ | awk -F: ‘{print $1}’ | sort | uniq -c | sort -nr | more 6.根据端口列进程 netstat -ntlp | grep 80 | awk ‘{print $7}’ | cut -d/ -f1 网站日志分析篇1（Apache）：1.获得访问前10位的ip地址 cat access.log|awk ‘{print $1}’|sort|uniq -c|sort -nr|head -10 cat access.log|awk ‘{counts[$(11)]+=1}; END {for(url in counts) print counts[url], url}’ 2.访问次数最多的文件或页面,取前20 cat access.log|awk ‘{print $11}’|sort|uniq -c|sort -nr|head -20 3.列出传输最大的几个exe文件（分析下载站的时候常用） cat access.log |awk ‘($7~/.exe/){print $10 &quot; &quot; $1 &quot; &quot; $4 &quot; &quot; $7}’|sort -nr|head -20 4.列出输出大于200000byte(约200kb)的exe文件以及对应文件发生次数 cat access.log |awk ‘($10 &gt; 200000 &amp;&amp; $7~/.exe/){print $7}’|sort -n|uniq -c|sort -nr|head -100 5.如果日志最后一列记录的是页面文件传输时间，则有列出到客户端最耗时的页面 cat access.log |awk ‘($7~/.php/){print $NF &quot; &quot; $1 &quot; &quot; $4 &quot; &quot; $7}’|sort -nr|head -100 6.列出最最耗时的页面(超过60秒的)的以及对应页面发生次数 cat access.log |awk ‘($NF &gt; 60 &amp;&amp; $7~/.php/){print $7}’|sort -n|uniq -c|sort -nr|head -100 7.列出传输时间超过 30 秒的文件 cat access.log |awk ‘($NF &gt; 30){print $7}’|sort -n|uniq -c|sort -nr|head -20 8.统计网站流量（G) cat access.log |awk ‘{sum+=$10} END {print sum/1024/1024/1024}’ 9.统计404的连接 awk ‘($9 ~/404/)’ access.log | awk ‘{print $9,$7}’ | sort 10. 统计http status cat access.log |awk &apos;{counts[$(9)]+=1}; END {for(code in counts) print code, counts[code]}&apos; cat access.log |awk &apos;{print $9}&apos;|sort|uniq -c|sort -rn 10.蜘蛛分析，查看是哪些蜘蛛在抓取内容。 /usr/sbin/tcpdump -i eth0 -l -s 0 -w - dst port 80 | strings | grep -i user-agent | grep -i -E &apos;bot|crawler|slurp|spider&apos; 网站日分析2(Squid篇）按域统计流量cat squid_access.log.tar.gz| awk &apos;{print $10,$7}&apos; |awk &apos;BEGIN{FS=&quot;[ /]&quot;}{trfc[$4]+=$1}END{for(domain in trfc){printf &quot;%st%dn&quot;,domain,trfc[domain]}}&apos; 安全篇：(ssh lastb)ssh日志中失败登录的IP，取出来 /var/log/secure awk &apos;/Failed PASSWORD/{IP[$(NF-3)]++}END{for(i in IP){print i,IP[i]}&apos; /var/log/secure 1、文件ip_list.txt如下格式，请提取”.magedu.com”前面的主机名部分并写 入到回到该文件中 1 blog.magedu.com 2 www.magedu.com … 999 study.magedu.com 2、统计/etc/fstab文件中每个文件系统类型出现的次数 [root@centos7 ~]cat /etc/fstab | awk &apos;/^UUID/{print $3}&apos;|sort|uniq -c |sort -nr 3 xfs 1 swap [root@centos7 ~]cat /etc/fstab | awk &apos;/^UUID/{file[$3]++}END{for(i in file){print i,file[i]}}&apos; swap 1 xfs 3 3、统计/etc/fstab文件中每个单词出现的次数 root@centos7 ~]cat /etc/fstab |awk &apos;{for(i=1;i&lt;=NF;i++){count[$i]++}}END{for(j in count){print j,count[j]}}&apos; man 1 4、提取出字符串Yd$C@M05MB%9&amp;Bdh7dq+YVixp3vpw中的所有数字 [root@centos7 ~]echo &quot;Yd$C@M05MB%9&amp;Bdh7dq+YVixp3vpw&quot;|awk &apos;gsub(/[^0-9]/,&quot;&quot;,$0)&apos; 05973 [root@centos7 ~]echo &quot;Yd$C@M05MB%9&amp;Bdh7dq+YVixp3vpw&quot;|tr -dc &quot;[0-9]&quot; 05973 5、有一文件记录了1-100000之间随机的整数共5000个，存储的格式 100,50,35,89…请取出其中最大和最小的整数 6、解决DOS攻击生产案例：根据web日志或者或者网络连接数，监控当某个IP 并发连接数或者短时内PV达到100，即调用防火墙命令封掉对应的IP，监控频 率每隔5分钟。防火墙命令为：iptables -A INPUT -s IP -j REJECT [root@centos7 ~]awk &apos;{access[$1]++}END{for(i in access){if(access[i]&gt;=1){print i,access[i]}}}&apos; /var/log/httpd/access_log 192.168.34.101 9 192.168.34.103 13 只过滤出IP，监控任务可以写到计划任务里， 或者用内置函数system[&quot;iptables&quot;]调用？ 7、将以下文件内容中FQDN取出并根据其进行计数从高到低排序 http://mail.magedu.com/index.html http://www.magedu.com/test.html http://study.magedu.com/index.html http://blog.magedu.com/index.html http://www.magedu.com/images/logo.jpg http://blog.magedu.com/20080102.html [root@centos7 ~]#cat f5|sed -nr &apos;s@.*//([^/]+)/.*@\1@p&apos;|sort|uniq -c|sort -nr 2 www.magedu.com 2 blog.magedu.com 1 study.magedu.com 1 mail.magedu.com [root@centos7 ~]#cat f5|awk -F&quot;/&quot; &apos;{print $3}&apos;|sort|uniq -c|sort -nr 2 www.magedu.com 2 blog.magedu.com 1 study.magedu.com 1 mail.magedu.com 8、将以下文本以inode为标记，对inode相同的counts进行累加，并且统计出 同一inode中，beginnumber的最小值和endnumber的最大值 inode|beginnumber|endnumber|counts| 106|3363120000|3363129999|10000| 106|3368560000|3368579999|20000| 310|3337000000|3337000100|101| 310|3342950000|3342959999|10000| 310|3362120960|3362120961|2| 311|3313460102|3313469999|9898| 311|3313470000|3313499999|30000| 311|3362120962|3362120963|2| 输出的结果格式为： 310|3337000000|3362120961|10103| 311|3313460102|3362120963|39900| 106|3363120000|3368579999|30000| awk -F&apos;|&apos; &apos;NR==1{print;next}{a[$1]?(a[$1]&gt;$2?a[$1]=$2:0):(a[$1]=$2);b[$1]?(b[$1]&lt;$3?b[$1]=$3:0):(b[$1]=$3);c[$1]+=$4} END{l=asorti(a,d);for(i=1;i&lt;=l;i++)print d[i] FS a[d[i]] FS b[d[i]] FS c[d[i]] FS}&apos; file [解析] 第一行直接打印。从第2行开始以$1为下标，建立3个数组，比较出$2的最小值，$3的最大值，然后把$4进行累加，最后进行排序后依次取出各项值。 这其中运用了三目运算的嵌套，跟我们 if(){if(){}}else{} 的使用是一个道理，不要认为复杂，如果觉得模糊不清，仔细读懂流程控制。 9.统计字符串中每个字母出现的次数abcdaabbccdd [root@centos7 mnt]#echo abcdaabbccdd | awk &apos;gsub(//,&quot;.&quot;,$0)&apos;|awk -F&quot;.&quot; &apos;{for(i=2;i&lt;=NF-1;i++)num[$i]++}END{for(j in num){print j,num[j]}}&apos; a 3 b 3 c 3 d 3 下面的写法在双引号前一定要加一个空格才能匹配出来 或者用单引号，但是也需要在前面几个空格 [root@centos7 mnt]#echo abcdaabbccdd|awk -F &quot;&quot; &apos;{for(i=1;i&lt;=NF;i++)x[$i]++}END{for(i in x){print i,x[i]}}&apos; a 3 此处一定有个空格 b 3 c 3 d 3 或者用单引号，但是也需要在前面几个空格 [root@mini7-1 ~]#echo abcdaabbccdd |awk -F &apos;&apos; &apos;{for(i=2;i&lt;=NF-1;i++)num[$i]++}END{for(j in num){print j,num[j]}}&apos; a 2 b 3 c 3 d 2 root@centos7 mnt]#echo abcdaabbccdd | grep -o &quot;[a-z]&quot;|sort|uniq -c 3 a 3 b 3 c 3 d 10.面试题：取出/etc/fstab中的挂载目录 [root@node7-1 data]#cat /etc/fstab | awk -F &quot;[ ]/?&quot; &apos;/^UUID/{print $2}&apos; boot data swap [root@node7-1 data]#cat /etc/fstab | awk -F &quot;[ ]/|[ ]&quot; &apos;/^UUID/{print $2}&apos; boot data swap AWK中的输入分隔符我们可以使用两次awk -F命令，每次分别指定一个分隔符来进行操作，但是这样太麻烦，还有更简单的方法，即一次指定多个分隔符。要一次指定多个分隔符，需要将分隔符用中括号[]包裹起来，如果多个分隔符中有至少一个特殊字符，那么还需要在中括号外加上双引号或者单引号，并且使用两个或两个以上的\将其进行转义 $、^、(、)、[、]、?、.、| 示例1： [root@node7-1 data]cat b.txt ssh:user1@192.168.1.10 ssh:user2@192.168.1.11 ssh:user3@192.168.1.12 1.取user和IP [root@node7-1 data]awk -F [:@] &apos;{print $2,$3}&apos; b.txt user1 192.168.1.10 user2 192.168.1.11 user3 192.168.1.12 2.上面b.txt中的：和@换成^和|，又该怎么取？ [root@node7-1 data]awk -F &apos;[\\^\\|]&apos; &apos;{print $2,$3}&apos; b.txt user1 192.168.1.10 user2 192.168.1.11 user3 192.168.1.12 示例2： 示例1： george[walker]bush william[jefferson]clinton 如果要打印出由分隔符[和]分隔的三段数据，即可以分别使用两个或两个以上的\对[和]进行转义，如下所示 方法一： awk -F &apos;[\\[\\]]&apos; &apos;{ print $1,$2,$3}&apos; name.txt george walker bush william jefferson clinton 方法二： awk -F &apos;[][]&apos; &apos;{ print $1,$2,$3}&apos; name.txt george walker bush william jefferson clinton 11.取出文本中的姓名和FQDN，awk的方法参照文章最上面的链接文章 [root@node7-1 data]cat a.txt xiaoming\t20\thttp://sougou.com xiaohua\t25\thttp://www.baidu.com xiaodong\t30\thttp://www.jidong.com 方法一：用awk取，以\t做分隔符，但是在表示方法上比较特殊 root@node7-1 data]awk -F&apos;\\\\t&apos; &apos;{print $1,$3}&apos; a.txt xiaoming http://sougou.com xiaohua http://www.baidu.com xiaodong http://www.jidong.com 分析： 第一个给第二个转义，第三个给第四个转义，传给awk对就是\\t，awk再将\\解释成\t 方法二：用awk和sed取 [root@node7-1 data]awk -F&apos;\\&apos; &apos;{print $1,$3}&apos; a.txt |sed -r &apos;s@(.* )t(.*)@\1\2@&apos; xiaoming http://sougou.com xiaohua http://www.baidu.com xiaodong http://www.jidong.com 12.扩展11题的内容把t换成$,又该如何取？ [root@node7-1 data]cat a.txt xiaoming\$20\$http://sougou.com xiaohua\$25\$http://www.baidu.com xiaodong\$30\$http://www.jidong.com 方法一：还是只用awk来取 [root@node7-1 data]awk -F&apos;\\\\\\$&apos; &apos;{print $1,$3}&apos; a.txt xiaoming http://sougou.com xiaohua http://www.baidu.com xiaodong http://www.jidong.com 分析： 1.\\$是转义$的 2.前四个\\\\是转义\的 方法二：awk和sed取 [root@node7-1 data]awk -F&apos;\\&apos; &apos;{print $1,$3}&apos; a.txt |sed -r &apos;s@(.* )\$(.*)@\1\2@&apos; xiaoming http://sougou.com xiaohua http://www.baidu.com xiaodong http://www.jidong.com 分析：先用awk用\做分隔符来取，再用sed取，sed中的$需要转义]]></content>
      <categories>
        <category>linux基础</category>
      </categories>
      <tags>
        <tag>文本处理工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[旧事-大好河山]]></title>
    <url>%2F2017%2F10%2F01%2F%E6%97%A7%E4%BA%8B-%E5%A4%A7%E5%A5%BD%E6%B2%B3%E5%B1%B1%2F</url>
    <content type="text"><![CDATA[输入密码,PC:Enter查看,Phone:输入法换行查看. U2FsdGVkX19uCNPAn4QtDE1q38X8DdZKeawt4no2wVj+vRV/+Kmu8GZ0FrQ+K7pA+nUuK/gsA60gqLkYuKDMr+cCGywUHcJQCK6bSBVG79ouFJo6ZJqVNCeQ8hKvq0b8Y4MM06Ogdnkq8zVISzHYpoTVdKtu5UyyW5dzni+47kT69AWstK4SVE0rAMvHxqMeL7YBruoViVFsq3NBQbsYNn8QKa9GjT9m7w22FZ94FyP/phVMnjWbflQFEnFg6eqfTOgeMeZ48ytLM2cYqPa06FxAk9J0QB4EsGggfNzlnMR6+9IdyXXhFfv4Uyn8Jcqsr/DJOXKSUd58qJpLUhdPKVUxR1QwlFPM0pUckDrafM56b1MJ9owCoyUm0ngHwOGR]]></content>
      <categories>
        <category>旧事，杂记</category>
      </categories>
      <tags>
        <tag>旧事，杂记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cobbler]]></title>
    <url>%2F2017%2F05%2F06%2FCobbler%2F</url>
    <content type="text"><![CDATA[在服务器很多的情况下，用U盘或者光盘安装时，就显得有点吃力了，虽然可以用PXE进行安装，但是PXE不支持UEFI所以现在更多使用Cobbler来实现自动化部署. Cobbler工作原理 系统自动安装之-cobbler之前写过PXE自动化安装的文档，但是PXE只支持BIOS，不支持UEFI:PXE系统自动化部署Cobbler可以同时支持BIOS和UEFI两种：基于PXE技术为基础的整合 BIOS+MBR：分区最多支持2T的UEFI+GPT：分区可以支持大于2T的分区 在之前的工作中，需要建立大于2T的分区时，PXE安装就不合适了 cobbler:具有图形化管理界面的工具cobbler是什么？ 实际上是把PXE技术用python代码做了封装，形成了相对完整的自动化安装， 但是实现PXE的原有的httpd，DHCP，tftp服务还是需要提前安装的。 Cobbler: 快速网络安装linux操作系统的服务，支持众多的Linux发行版：Red Hat、 Fedora、CentOS、Debian、Ubuntu和SuSE，也可以支持网络安装windows PXE的二次封装，将多种安装参数封装到一个菜单 Python编写 提供了CLI和Web的管理形式 安装cobbler：EPEL源安装包 cobbler 基于EPEL源 cobbler 服务集成 前面说过因为是基于PXE的,安装cobbler因为有依赖性，会自动安装下面的服务 PXE DHCP rsync Httpd DNS Kickstart syslinux tftp-server IPMI 电源管理 启动cobbler:依赖于httpd，要想启动cobbler,必须先启动httpd systemctl start httpd tftp dhcp systemctl start cobblerd 然后再检查cobbler环境 cobbler check cobbler的相关配置文件安装：yum install cobbler dhcp 配置文件目录 /etc/cobbler /etc/cobbler/settings : cobbler 主配置文件(下文会对该文件修改4行) /etc/cobbler/iso/: iso模板配置文件 /etc/cobbler/pxe: pxe模板文件 /etc/cobbler/power: 电源配置文件 /etc/cobbler/user.conf: web服务授权配置文件 /etc/cobbler/users.digest: web访问的用户名密码配置文件 /etc/cobbler/dhcp.template : dhcp服务器的的配置末班 /etc/cobbler/dnsmasq.template : dns服务器的配置模板 /etc/cobbler/tftpd.template : tftp服务的配置模板 /etc/cobbler/modules.conf : 模块的配置文件 数据目录 /var/lib/cobbler/config/: 用于存放distros，system，profiles 等信息配置文件 /var/lib/cobbler/triggers/: 用于存放用户定义的cobbler命令 /var/lib/cobbler/kickstart/: 默认存放kickstart文件 /var/lib/cobbler/loaders/: 存放各种引导程序 镜像目录 /var/www/cobbler/ks_mirror/: 导入的发行版系统的所有数据 /var/www/cobbler/images/ : 导入发行版的kernel和initrd镜像用于远程网络启动 /var/www/cobbler/repo_mirror/: yum 仓库存储目录 日志目录 /var/log/cobbler/installing: 客户端安装日志 /var/log/cobbler/cobbler.log : cobbler日志 cobbler命令介绍cobbler commands介绍 cobbler check 核对当前设置是否有问题 cobbler list 列出所有的cobbler元素 cobbler report 列出元素的详细信息 cobbler sync 同步配置到数据目录,更改配置最好都要执行下 cobbler reposync 同步yum仓库 cobbler distro 查看导入的发行版系统信息 cobbler system 查看添加的系统信息 cobbler profile 查看配置信息 cobbler重要的参数:及下面需要修改的4行内容/etc/cobbler/settings中重要的参数设置 default_password_crypted: &quot;$1$gEc7ilpP$pg5iSOj/mlxTxEslhRvyp/&quot; server: 192.168.34.17 next_server: 192.168.34.17 server：&lt;cobbler服务器的 IP 地址&gt; manage_dhcp: 1 manage_tftpd：1 pxe_just_once：1 cobbler环境检查：先启动cobbler服务再检查1.先启动cobbler服务，再用cobbler check 进行检查 执行Cobbler check命令会报如下异常 1 : The ‘server’ field in /etc/cobbler/settings must be set to something other than localhost, or kickstarting features will not work. This should be a resolvable hostname or IP for the boot server as reachable by all machines that will use it. 2 : For PXE to be functional, the ‘next_server’ field in /etc/cobbler/ settings must be set to something other than 127.0.0.1, and should match the IP of the boot server on the PXE network. 3 : some network boot-loaders are missing from /var/lib/cobbler/loaders, you may run ‘cobbler get-loaders’ to download them, or, if you only want to handle x86/x86_64 netbooting, you may ensure that you have installed a recent version of the syslinux package installed and can ignore this message entirely. Files in this directory, should you want to support all architectures, should include pxelinux.0, menu.c32, elilo.efi, and yaboot. The ‘cobbler get-loaders’ command is the easiest way to resolve these requirements. 4 : change ‘disable’ to ‘no’ in /etc/xinetd.d/rsync 5 : comment ‘dists’ on /etc/debmirror.conf for proper debian support 6 : comment ‘arches’ on /etc/debmirror.conf for proper debian support 7 : The default password used by the sample templates for newly installed machines (default_password_crypted in /etc/cobbler/settings) is still set to ‘cobbler’ and should be changed, try: “openssl passwd -1 -salt ‘random-phrase-here’ ‘your-password-here’” to generate new one 8 : fencing tools were not found, and are required to use the (optional) power management features. install cman or fence-agents to use them Cobbler的8项报错解决方法错误信息都是在/etc/cobbler/settings文件改了4行，在下文体现 执行Cobbler check报错解决方式 第一个错误解决方法： 1.修改/etc/cobbler/settings文件中的server参数的值为提供cobbler服务的主机相 应的IP地址或主机名：384行，然后重新启动cobbler server: 192.168.34.107 2.修改/etc/cobbler/settings文件中的next_server参数的值为提供PXE服务的主机 相应的IP地址:272行，指定的tftp的服务器地址 next_server: 192.168.34.107 3.执行cobbler get-loaders和cobbler sync； 如果当前节点可以访问互联网，执行&quot;cobbler get-loaders&quot;命令即可； cobbler会自动通过互联网把最小化的系统启动文件下载下来放到 /var/lib/cobbler/loaders/下，再通过&quot;cobbler sync&quot;命令同步到/var/lib/tftpboot/下 4.cobbler认为是在centos6上，tftp服务是依赖于xinetd的，提示启动xinetd,在 这里，由于是在cnetos7上安装的，这项可以忽略 5.执行“chkconfig rsync on”命令即可 4,5,6项如果是在centos7上安装的cobbler是不需要修改也可以启动的 7.第7项是说安装的操作系统密码cobbler事先已经帮忙设置好了，但是不安全，需要自己 去修改一个自定义的密码(通过openssl passwd -1)生成：101行 default_password_crypted：修改成自己设置的密码 备注：在这个提示里，还有一项建议修改，之前用PXE的时候，搭建的DHCP服务，dhcpd.conf 文件时通过模板生成的，但是在cobbler中，可以通过colbber来生成，但是需要改 /etc/cobbler/settings一项配置：242行 manage_dhcp: 0 改成 manage_dhcp: 1 再通过cobbler自带的dhcp模板：/etc/cobbler/dhcp.template，生成dhcpd.conf 只需要把这个模板改一下就行了，而不用想PXE那样通过模板生成dhcpd.conf文件 把dhcp.template里的网段地址改一下 subnet 192.168.34.0 netmask 255.255.255.0 { option subnet-mask 255.255.255.0; range dynamic-bootp 192.168.34.20 192.168.34.100; 再通过cobbler sync重新生成/etc/dhcp/dhcpd.conf,并且会开启DHCP服务 上面的报错全部解决之后，且httpd,tftp，dhcp,cobbler都启动之后，实际上就已经具备了雏形了/var/lib/tftpboot的目录结构:上面的第三步完成后就生成下面的所有文件了[root@mini7-1 tftpboot]#tree /var/lib/tftpboot /var/lib/tftpboot ├── boot │ └── grub │ └── menu.lst ├── etc ├── grub │ ├── efidefault │ ├── grub-x86_64.efi │ ├── grub-x86.efi │ └── images -&gt; ../images ├── images ├── images2 ├── memdisk ├── menu.c32 ├── ppc ├── pxelinux.0 ├── pxelinux.cfg │ └── default ├── s390x │ └── profile_list └── yaboot 接下来就要准备yum源了，只不过不用像PXE那样，不用自己搭建httpd服务，创建centos各个版本的目录了，通过cobbler集成命令将centos6和centos7的光盘文件导入，实际上就是拷贝到cobbler对应的目录下，查看一下磁盘空间，因为要导入两个光盘(centos6&amp;centos7)cobbler命令的选项： cobbler &lt;distro|profile|system|repo|image|mgmtclass|package|file&gt; ... [add|edit|copy|getks*|list|remove|rename|report] [options|--help] cobbler &lt;aclsetup|buildiso|import|list|replicate|report|reposync|sync|validateks|version|signature|get-loaders|hardlink&gt; [options|--help] 这里用import选项将6和7的光盘导入cobbler的主机上 mount /dev/sr0 /mnt/ ---&gt;挂载centos7光盘 mount /dev/sr1 /media ---&gt;挂载centos6光盘 cobbler import --path=/mnt/ --name=Centos-7.5-x86_64 --arch=x86_64 cobbler import --path=/media/ --name=Centos-6.10-x86_64 --arch=x86_64 cobbler会把光盘拷贝到/var/www/cobbler/下分开存放，目录结构如下 [root@mini7-1 cobbler]#tree -d /var/www/cobbler /var/www/cobbler ├── images │ ├── Centos-6.10-x86_64 │ └── Centos-7.5-x86_64 ├── ks_mirror │ ├── Centos-6.10-x86_64 │ │ ├── EFI │ │ │ └── BOOT │ │ ├── images │ │ │ └── pxeboot │ │ ├── isolinux │ │ ├── Packages │ │ └── repodata │ ├── Centos-7.5-x86_64 │ │ ├── EFI │ │ │ └── BOOT │ │ │ └── fonts │ │ ├── images │ │ │ └── pxeboot │ │ ├── isolinux │ │ ├── LiveOS │ │ ├── Packages │ │ └── repodata │ └── config ├── links │ ├── Centos-6.10-x86_64 -&gt; /var/www/cobbler/ks_mirror/Centos-6.10-x86_64 │ └── Centos-7.5-x86_64 -&gt; /var/www/cobbler/ks_mirror/Centos-7.5-x86_64 ├── localmirror ├── misc ├── pub ├── rendered ├── repo_mirror └── svc 拷贝完，cobbler sync再同步一次 然后cobbler会把/var/lib/tftpboot/pxelinux.cfg/default文件自动更新，生成新的菜单 DEFAULT menu PROMPT 0 MENU TITLE Cobbler | http://cobbler.github.io/ TIMEOUT 200 TOTALTIMEOUT 6000 ONTIMEOUT local LABEL local MENU LABEL (local) MENU DEFAULT LOCALBOOT -1 LABEL Centos-6.10-x86_64 kernel /images/Centos-6.10-x86_64/vmlinuz MENU LABEL Centos-6.10-x86_64 append initrd=/images/Centos-6.10-x86_64/initrd.img ksdevice=bootif lang= kssendmac text ks=http://192.168.34.107/cblr/svc/op/ks/profile/Centos-6.10-x86_64 ipappend 2 LABEL Centos-7.5-x86_64 kernel /images/Centos-7.5-x86_64/vmlinuz MENU LABEL Centos-7.5-x86_64 append initrd=/images/Centos-7.5-x86_64/initrd.img ksdevice=bootif lang= kssendmac text ks=http://192.168.34.107/cblr/svc/op/ks/profile/Centos-7.5-x86_64 ipappend 2 MENU end 此时，就可以用cobbler实现自动化安装了，但是不能自定义ks应答文件安装，下面会讲解如何自定义cobbler的应答文件 Cobbler中自定义应答文件之前在PXE安装时，是自定义的kickstart应答文件，在cobbler中如何实现？ 1.在cobbler中，应答文件是放在/var/lib/cobbler/kickstarts/中 2.把之前制作的ks6-mini.cfg，ks7-mini.cfg拷贝到此目录，但是拷贝完，但cobbler是无 法识别这些应答文件是对应的那个发行版本的，所以要绑定 3.将自定义的应答文件和安装版本进行绑定 4.这两个应答文件有一项需要修改 即url --url=这一项，要改成cobbler的yum源路径，$tree 这里只显示ks6-mini.cfg的详细信息 install url --url=http://192.168.34.103/centos/6/os/x86_64/ (PXE下的源路径) #httpd的yum源路径，要改成$tree 或者改成具体的地址：即光盘拷贝到cobbler的具体路径 http://192.168.34.107/cobbler/ks_mirror/Centos-6.10-x86_64/ lang en_US.UTF-8 keyboard us text #纯文本安装 reboot #安装完重启 network --onboot yes --device eth0 --bootproto dhcp --noipv6 rootpw --iscrypted $6$6zeqEsKimGywyY3J$ngSoHUIrLaydTMIwnYX9tg4HOeOCXogU03miaQIc8sOwmq5N6.fPk3Jrmouns8VOekZkn0YQjQkt JjA57WrZO0 firewall --service=ssh 关闭防火墙 authconfig --enableshadow --passalgo=sha512 selinux --enforcing 关闭selinux timezone Asia/Shanghai 时区信息 bootloader --location=mbr --driveorder=sda --append=&quot;crashkernel=auto rhgb quiet&quot; # The following is the partition information you requested # Note that any partitions you deleted are not expressed # here so unless you clear all partitions first, this is # not guaranteed to work clearpart --all 清空分区信息 zerombr 清空mbr #自定义的分区信息 part /boot --fstype=ext4 --size=1024 part / --fstype=ext4 --size=50000 part /data --fstype=ext4 --size=30000 part swap --size=2048 #自定义的分区信息 %packages @core %end 只安装最基本的核心包，后面也可以加上安装后脚本 将KS和OS关联，生成启动新的菜单自定义的应答文件的绑定、删除、重命名、显示菜单栏对应的ks应答文件信息 在cobberl中 distro中记录的是cobbler中安装的发型版本对应的原文件的 [root@mini7-1 kickstarts]#cobbler distro list Centos-6.10-x86_64 Centos-7.5-x86_64 在cobberl中 profile是对应的各个发型版本的安装方法，就是安装启动界面的选择菜单栏，有多少个 就对应多少个菜单栏：如下图只有两个 [root@mini7-1 kickstarts]#cobbler profile list Centos-6.10-x86_64 Centos-7.5-x86_64 将自定义的应答文件和安装版本进行绑定：cobbler profile命令绑定 将ks6-mini.cfg和centos6进行绑定 cobbler profile add --name=centos-6.10-x86_64_mini --distro=Centos-6.10-x86_64 --kickstart=/var/lib/cobbler/kickstarts/ks6-mini.cfg cobbler profile add --name=centos-7.5-x86_64_mini --distro=Centos-7.5-x86_64 --kickstart=/var/lib/cobbler/kickstarts/ks7-mini.cfg 也可以删除应答文件： cobbler profile remove --name=Centos-6.10-x86_64 cobbler profile remove --name=Centos-7.5-x86_64 也可以修改带单名 cobbler profile rename --name=Centos-7.5-x86_64 --newname=centos-7.5-x86_64_desktop 查看菜单项对应的具体是哪个应答文件信息 cobbler profile report --name=centos-7.5-x86_64_mini /var/lib/tftpboot/pxelinux.cfg/default会自动生成新的菜单项，从cobbler profile list,也可以看出来 [root@mini7-1 tftpboot]#cobbler profile list Centos-6.10-x86_64 Centos-7.5-x86_64 centos-6.10-x86_64_mini centos-7.5-x86_64_mini cobbler的web管理实现此外cobbler是带有web管理界面的，不过需要安装web界面的包 yum install cobbler-web -y 配置文件： [root@mini7-1 kickstarts]#rpm -qf cobbler-web /etc/httpd/conf.d/cobbler_web.conf cobbler-web是经过http协议的，所以 安装完，需要重启httpd服务 然后看到对应的是443端口启动了 登录界面是，因为cobbler-web是https加密的/etc/cobbler/modules.conf 验证方法;/etc/cobbler/users.digest 记录的用户增加用户]]></content>
      <categories>
        <category>系统安装</category>
      </categories>
      <tags>
        <tag>运维，linux，windows</tag>
      </tags>
  </entry>
</search>
